\documentclass[12pt,a4paper]{amsart}

% ukazi za delo s slovenscino -- izberi kodiranje, ki ti ustreza
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[slovene]{babel}
\usepackage{lmodern} 
%\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{url}
%\usepackage[normalem]{ulem}
\usepackage[dvipsnames,usenames]{color}
\usepackage{eurosym}
\usepackage{nicefrac}
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}

\usepackage{mathtools}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}

% ne spreminjaj podatkov, ki vplivajo na obliko strani
\textwidth 15cm
\textheight 24cm
\oddsidemargin.5cm
\evensidemargin.5cm
\topmargin-5mm
\addtolength{\footskip}{10pt}
\pagestyle{plain}
\overfullrule=15pt % oznaci predlogo vrstico


% ukazi za matematicna okolja
\theoremstyle{definition} % tekst napisan pokoncno
\newtheorem{definicija}{Definicija}[section]
\newtheorem{primer}[definicija]{Primer}
\newtheorem{opomba}[definicija]{Opomba}

\renewcommand\endprimer{\hfill$\diamondsuit$}


\theoremstyle{plain} % tekst napisan posevno
\newtheorem{lema}[definicija]{Lema}
\newtheorem{izrek}[definicija]{Izrek}
\newtheorem{trditev}[definicija]{Trditev}
\newtheorem{posledica}[definicija]{Posledica}

\DeclareCaptionFormat{myformat}{#3}
\captionsetup[algorithm]{format = myformat}
% za stevilske mnozice uporabi naslednje simbole
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\newcommand{\Q}{\mathbb Q}


% ukaz za slovarsko geslo
\newlength{\odstavek}
\setlength{\odstavek}{\parindent}
\newcommand{\geslo}[2]{\noindent\textbf{#1}\hspace*{3mm}\hangindent=\parindent\hangafter=1 #2}


% naslednje ukaze ustrezno popravi
\newcommand{\program}{Finančna matematika} % ime studijskega programa: Matematika/Finan"cna matematika
\newcommand{\imeavtorja}{Mitja Mandić} % ime avtorja
\newcommand{\imementorja}{izred.~prof.~dr. Jaka Smrekar} % akademski naziv in ime mentorja
\newcommand{\naslovdela}{Iterativne numerične metode v posplošenih linearnih modelih}
\newcommand{\letnica}{2021} %letnica diplome


% vstavi svoje definicije ...




\begin{document}

% od tod do povzetka ne spreminjaj nicesar
\thispagestyle{empty}
\noindent{\large
UNIVERZA V LJUBLJANI\\[1mm]
FAKULTETA ZA MATEMATIKO IN FIZIKO\\[5mm]
\program\ -- 1.~stopnja}
\vfill

\begin{center}{\large
\imeavtorja\\[2mm]
{\bf \naslovdela}\\[10mm]
Delo diplomskega seminarja\\[1cm]
Mentor: \imementorja}
\end{center}
\vfill

\noindent{\large
Ljubljana, \letnica}
\pagebreak

\thispagestyle{empty}
\tableofcontents
\pagebreak

\thispagestyle{empty}
\begin{center}
{\bf \naslovdela}\\[3mm]

{\sc Povzetek}
\end{center}
% tekst povzetka v slovenscini
V delu obravnavamo numerične metode, ki se uporabljajo pri računanju cenilk največjega verjetja v posplošenih linearnih modelih. Za uvod si postavimo teoretične 
temelje z eksponentno družino, nato pa natančneje spoznamo logistični in probit model, za katera tudi izpeljemo enačbe verjetja. Dobljene enačbe komentiramo tudi v splošnem
in komentiramo, zakaj je smiselno uporabljatii t.i. kanonične modele. V delu, namenjem numeričnim metodam, najprej navedemo in dokažemo nekaj dejstev o Newtonovi metodi za računanje
ničel funkcij, ki jo nato prilagodimo za iskanje ekstremov funkcij verjetja. Vse zaključke preizkusimo in komentiramo tudi na praktičnih primerih.

\vfill
\begin{center}
{\bf Iterative numerical methods in generalized linear models}\\[3mm] % prevod slovenskega naslova dela
{\sc Abstract}
\end{center}
% tekst povzetka v anglescini


\vfill\noindent
{\bf Math. Subj. Class. (2010):} navedi vsaj eno klasifikacijsko oznako -- dostopne so na \url{www.ams.org/mathscinet/msc/msc2010.html}  \\[1mm]
{\bf Klju"cne besede:} navedi nekaj klju"cnih pojmov, ki nastopajo v delu  \\[1mm]
{\bf Keywords:} angle"ski prevod klju"cnih besed
\pagebreak



% tu se zacne besedilo seminarja
\section{Uvod}
V sodobnem svetu neomejenih podatkov, je njihovo obvladanje in koristno uporabljanje ključnega pomena. Velikokrat relacije med njimi niso vidne na 
prvi pogled, a zato niso nič manj pomembne. Posplošeni linearni modeli so orodje, s katerimi te povezave modeliramo in jih poskušamo razumeti. So ena osnovnih
metod statističnga raziskovanja. Vendar pa najenostavnejši modeli niso nujno najboljši in z zahtevnostjo naraste tudi problematika računanja. Tu pa nam na
pomoč priskočijo numerične metode, ki pa morajo biti dovolj robustne in hkrati čim enostavnejše za računanje.

V delu si bomo najprej ogledali linearni model, natančneje pa spoznali dva glavna modela za obdelavo podatkov, kjer so odgovori 0 ali 1 - binarnih podatkov, to sta 
\textit{logistični} in \textit{probit model}, uporabna za računanje verjetnosti dogodgkov. Za njihovo uporabo bomo razvili numerične algoritme in jih na koncu
preizkusili na podatkih o odpovedih tesnil vesoljskih poletov pred dobo \textit{Challengerja.}

\section{Eksponentna družina} 
Za uvod v nalogo si najprej definirajmo osnovo, na kateri bo kasneje temeljil eden glavnih zaključkov naloge. Predvsem nam bodo zaključki poglavja pomagali
pri posploševanju rezultatov. Slučajna spremenljivka $Y$ torej pripada \textit{enoparametrični eksponentni družini z disperzijskim parametrom,} če je njegova gostota glede na neko 
$\sigma-$končno mero oblike 
\begin{equation}
    f_{Y}(y; \theta, \phi) = \exp{\left(\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)\right)},
\end{equation}
za neke funkcije $a(\cdot), b(\cdot)\text{~in~}c(\cdot).$ Parametru $\theta$ pravimo \textit{kanonični} oziroma \textit{naravni} parameter,
$\phi$ pa imenujemo \textit{disperzijski parameter}.

Koristno je pogledati logaritem zgornje enačbe
\begin{align}
    \log f_{Y}(y; \theta, \phi) &= \frac{y\theta - b(\theta)}{a(\phi)} + c(y,\phi) \\
    \frac{\partial}{\partial \theta} \log f_{Y}(y;\theta, \phi) &= V_{\theta}(y) = \frac{y - b'(\theta)}{a(\phi)} \\
    \frac{\partial^2}{\partial \theta^2}\log f_{Y}(y; \theta, \phi) &= -\frac{b''(\theta)}{a(\phi)},
\end{align}
kjer funkcijo $V$ imenujemo \textit{funkcija zbira}, oziroma v angleščini \textit{score function.}


Dokažimo sedaj nekaj koristnih zvez, ki jih bomo uporabili v kasnejših izpeljavah.
%TU MANJKA TRDITEV
\begin{trditev} \label{izp}
    Naj bo Y slučajna spremenljivka, katere gostota pripada eksponentni družini. Potem za pričakovano vrednost in varianco veljata sledeči zvezi:
    \[
        \mathbb{E}(Y) = b'(\theta),~~Var(Y) = b''(\theta)a(\phi)
    \]
\end{trditev}
\begin{proof}
Za dokaz prve enakosti si poglejmo
\begin{align*}
    \mathbb{E}(V(Y)) &= \int f_{Y}(y)\frac{\partial}{\partial \theta}\log f_{Y}(y;\theta,\phi) \,dy = \int f_{Y}(y)\frac{1}{f_{Y}(y)}\frac{\partial f_{Y}(y)}{\partial\theta}\,dy \nonumber\\
    &=\int\frac{\partial f_{Y}(y)}{\partial \theta} \,dy= \frac{\partial}{\partial \theta}\int f_{Y}(y)\,dy = \frac{\partial}{\partial \theta}1 = 0,
\end{align*}
saj je $f_{Y}(y)$ gostota. V zgornji in sledečih zvezah bomo namesto $f_{Y}(y;\theta,\phi),$ kjer to ne bo vodilo v dodatne težave, pisali kar $f_{Y}(y)$
Od tu sledi
\begin{align*}
    \mathbb{E}(V(y)) &= \mathbb{E}(\frac{Y - b'(\theta)}{a(\phi)}) = 0 \\
    \mathbb{E}(Y) &= b'(\theta)
\end{align*}
Za drugo pa si oglejmo
\begin{align*}
    \mathbb{E}(\frac{\partial}{\partial\theta}V(Y) + V(Y)^2) &= \int f_{Y}(y)\left(\frac{\partial}{\partial \theta} \left(\frac{1}{f_{Y}(y)}\frac{\partial f_{Y}(y)}{\partial\theta}\right) + \left(\frac{\partial}{\partial\theta}\log f_{Y}(y)\right)^2\right) \,dy \\
    &=\int f_{Y}(y)\left(-\frac{1}{f_{Y}(y)^2}\left(\frac{\partial f_{Y}(y)}{\partial\theta}\right)^2 + \frac{1}{f_{Y}(y)}\frac{\partial^2f_{Y}(y)}{\partial\theta^2} + \left(\frac{1}{f_{Y}(y)}\frac{\partial f_{Y}(y)}{\partial\theta}\right)^2  \right)\,dy\\    
    &=\int \frac{\partial^2}{\partial\theta^2} f_{Y}(y) \,dy = \frac{\partial^2}{\partial\theta^2}\int f_{Y}(y)\,dy = 0
\end{align*}
Spet uporabimo prej izpeljane zveze in dobimo
\begin{gather*}
    \frac{\partial^2}{\partial\theta^2}\log f_{Y}(y) = \frac{\partial}{\partial\theta}\left(\frac{y-b'(\theta)}{a(\phi)}\right) = -\frac{b''(\theta)}{a(\phi)},~~\mathbb{E}\left(-\frac{b''(\theta)}{a(\phi)}\right) = -\frac{b''(\theta)}{a(\phi)} \\
    \mathbb{E}\left(V(Y)^2\right) = \mathbb{E}\left(\left(\frac{Y-b'(\theta)}{a(\phi) }\right)^2\right) = \frac{1}{a(\phi)^2}\mathbb{E}((Y - \mathbb{E}(Y))^2) = \frac{1}{a(\phi)^2}Var(Y),
\end{gather*}
in po zgoraj dokazani enakosti za funkcijo zbira torej velja
\begin{align*}
    -\mathbb{E}\left(\frac{\partial^2}{\partial\theta^2}\log f_{Y}(y)\right) &= \mathbb{E}\left(V(Y)^2\right) \\
    \frac{b''(\theta)}{a(\phi)} &= \frac{1}{a(\phi)^2}Var(Y) \\
    \mathrm{Var}(Y) &= a(\phi)b''(\theta)
\end{align*}
\end{proof}
Zgornja trditev nam torej pove, da lahko pričakovano vrednost in varianco porazdelitve iz eksponentne družine, z nekaj odvajanja, preberemo kar iz gostote - izognemo se integriraciji, iz zadnje
zveze pa vidimo zakaj se parametru $\phi$ reče ravno disperzijski parameter.

Pričakovano vrednost kvadrata funkcije zbira v splošnem imenujemo \textit{Fisherjeva informacija}, $\mathrm{FI}(\theta) = \mathbb{E}((V(Y))^2),$ izpeljano zvezo, ki poveže funkcijo zbira in njene odvode pa \textit{informacijska enakost.}
Uporabnost teh zvez bo postala jasna v sledečih poglavjih.

Oglejmo si sedaj nekaj primerov porazdelitev eksponentne družine:
\begin{itemize}
    \item \textbf{Normalna porazdelitev.} Normalno porazdeljena slučajna spremenljivka $Y \sim N(\mu, \sigma^2)$
    ima gostoto $f_{Y}(y) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(y-\mu)^2}{2\sigma^2}\right)}.$
    Če zgornjo enačbo logaritmiramo dobimo
    \begin{align*}
        \log f_{Y}(y;\mu, \sigma) &= \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) -\frac{(y-\mu)^2}{2\sigma^2} = -\frac{1}{2}\log(2\sigma^2\pi) - \frac{y^2 - 2\mu y + \mu^2}{2 \sigma^2}    \\ \nonumber
                            &= \frac{y\mu-\nicefrac{\mu^2}{2}}{\sigma^2} - \frac{1}{2}\left(\frac{y^2}{\sigma^2} + \log(2\pi\sigma^2)\right) 
    \end{align*}
    Od tu preberemo zgoraj definirane vrednosti
    \[
        \theta = \mu,~\phi = \sigma^2,~a(\sigma^2)=\sigma^2,~b(\mu) = \frac{\mu^2}{2}
    \]
    in iz trditve \ref{izp} sledijo zaključki
    \[
        \mathbb{E}(Y) = b'(\mu) = \mu \text{ in } \mathrm{Var}(Y) = a(\sigma^2)b''(\mu) = \sigma^2.
    \]

    \item \textbf{Binomska porazdelitev.} Imejmo binomsko porazdeljeno slučajno spremenljivko $Y \sim \mathrm{Bin}(n,p).$ Izrazimo 
    \[
        \mathrm{P}(Y = y) = \binom{n}{y}p^y(1-p)^{n-y} = \exp\left(y\log(\frac{p}{1-p}) + n\log(1-p) + \log{\binom{n}{k}}\right),
    \]
    od koder direktno sledi
    \[
        \theta = \log\frac{p}{1-p},~b(\theta) = n\log(1 + e^{\theta}),~a(\phi) = 1,
    \]
    in opazimo da je tokrat naravni parameter $\log\frac{p}{1-p},$ kar imenujemo tudi \textit{logit} verjetnosti.
\end{itemize}

%Poleg omenjenih, spadajo v eksponentno družino z disperzijskim parametrom še Gamma, $\chi^2,$ eksponenta, Poissonova in kar nekaj drugih porazdelitev.
\section{Posplošeni linearni modeli}

\subsection{Sestavni deli posplošenega linearnega modela}
Vsak posplošeni linearni model sestavljajo trije deli: \textit{slučajni del} je slučajni vektor $Y$ in njegova porazdelitev, 
\textit{sistematični del} predstavlja relacijo med pojasnjevalnimi spremenljivkami, \textit{povezovalna funkcija} pa transformira $\mathbb{E}(Y)$, da se ta
bolje prilega podatkom. V nalogi bomo proučevali vektor $Y = (Y_{1},Y_{2},\ldots,Y_{n}),$ kjer so komponente neodvisne slučajne spremenljivke iz enoparametrične
eksponentne družine z disperzijskim parametrom.

\subsubsection{Slučajni del}
\textit{Slučajni del} privzame porazdelitev slučajnega vektorja $Y$, pri čemer privzemamo tudi neodvisnost komponent. Porazdelitev $Y$
privzemamo odvisno od podatkov; mnogokrat je ,,binarna``, torej ima dve možni vrednosti - ,,uspeh`` ali ,,neuspeh``. Splošneje je lahko izid tudi 
število uspehov v fiksnem številu poskusov. V takih primerih privzamemo binomsko porazdelitev. $Y$ nam lahko meri tudi števne podatke, naprimer 
koliko zabav je obiskal študent v preteklem mesecu. Seveda pa lahko $Y$ predstavlja tudi zvezne podatke,
v tem primeru lahko privzamemo normalno porazdelitev (ali pa kakšno drugo zvezno porazdelitev).
\subsubsection{Sistematični del} 
\textit{Sistematična komponenta} posplošenega linearnega modela poda relacije med pojasnjevalnimi spremenljivkami $x_{ij}$. Te nastopajo 
linearno, torej je sistematični del enak
\[
\beta_{0} + x_{i1}\beta_{1} + x_{i2}\beta_{2} + \ldots + x_{ip}\beta_{p}
\]
\subsubsection{Povezovalna funkcija}
Tretji del posplošenega linearnega modela je \textit{povezovalna funkcija}, ta nam poda funkcijo $g(\cdot)$ med slučajno komponento
in sistematičnim delom. Če označimo $\mu = \mathbb{E}(Y)$, je
\[
    g(\mu) = \beta_{0} + x_{i1}\beta_{1} + x_{i2}\beta_{2} + \ldots + x_{ip}\beta_{p}
\]
Najenostavnejša taka funkcija je kar identiteta, torej $g(\mu) = \mu$. Ta nam torej da linearno povezavo med pojasnjevalnimi spremenljivkami 
in pričakovano vrednostjo naših slučajnih spremenljivki. To je ena od oblik regresije za zvezne podatke.
Mnogokrat pa linearna relacija ni primerna - fiksna sprememba pojasnjevalnih spremenljivk ima lahko večji vpliv, če je pričakovana vrednost 
bližje 0, kot če je bližje 1. Recimo, da je $\pi$ verjetnost, da bo oseba kupila nov avto, ko je njen dohodek enak $x$. Sprememba v dohodku
za 10.000\euro~ima manjši vpliv, če je dohodek 1.000.000\euro,~kot če je 50.000\euro.
Takrat je smiselno uporabiti kakšno drugo povezovalno funkcijo, ki dopušča tudi nelinearne kombinacije pojasnjevalnih
spremenljivk. Naprimer, $g(\mu) = \log(\mu)$ modelira
pričakovano vrednost logaritma. Smiselno jo je uporabiti, če pričakovana vrednost ne more zavzeti negativnih vrednosti. Takemu modelu rečemo
\textit{log-linearen} model.
Spet druga povezovalna funkcija je $\mathrm{logit}(\mu) = \log(\frac{\mu}{1-\mu})$, ki nam modelira logaritem deležev - smiselno jo je uporabiti, 
ko $\mu$ ne zavzame vrednosti izven $(0,1)$, torej ko imamo opravka z verjetnostmi. Takemu modelu rečemo logistični model.
%\subsection{Primeri posplošenih linearnih modelov}
\subsection{Točkovno ocenjevanje}
Preden se natančneje posvetimo posplošenim linearnim modelom, si oglejmo dve najbolj znani metodi za ocenjevanje parametrov.
Najprej si definirajmo nekaj pojmov, ki jih bomo uporabljali v nadaljnjih poglavjih.

\textit{Cenilka} za realnoštevilsko karatkreristiko $c$ proučevane porazdelitve je funkcija vzorca $T=T(X_{1},\ldots,X_{n})$, s katero ocenjujemo $c.$
Ta cenilka je \textit{nepristranska}, če za porazdelitev vzorca $F$ velja $\mathbb{E}(T(X_{1},\ldots,X_{n})) = c(F).$
Imejmo sedaj zaporedje cenilk za vzorce velikosti $n=1,2,\ldots.$ To zaporedje je \textit{dosledno}, če v verjetnosti konvergira h konstanti $c(F).$

Če povzamem z drugimi besedami; nepristranska cenilka nam v povprečju vrne pravi rezultat, dosledna cenilka pa z večjim vzorcem vrne rezultat vedno
bližje ocenjevani karakteristiki.

\subsubsection{Metoda momentov}
Metodo momentov je Čebišev leta 1887 predstavil v svojem dokazu centralnega limitnega izreka. V splošnem ni tako uporabna kot spodaj opisana metoda največjega
verjetja, je pa precej enostavna za računanje brez računalnika. Če malce karikiramo, lahko idejo metode momentov povzamemo v \:
,,vse kar se da izraziti z momenti, ocenimo s cenilkami momentov.``

V splošnem z metodo momentov postopamo takole: če je ocenjevano karakteristiko proučevane slučajne spremenljivke $c(X)$ mogoče izraziti kot funkcijo 
momentov,t.j. če v danem modelu ti momenti obstajajo, 
\[
    c(X) = g(m_{1}(X), m_{2}(X),\ldots,m_{r}(X)),
\] za neko funkcijo g, potem $c(X)$ ocenjujemo s cenilko $g(\hat{m}_{1},\ldots,\hat{m}_{r}), $kjer je $\hat{m}_{k} = \frac{1}{n}\sum_{i=1}^{n}X^{k}_{i}$ . Če je g zvezna, dobimo dosledno cenilko.

\subsubsection{Metoda največjega verjetja} %mislim da štima
Imejmo parametrični model s prostorom parametrov $\Theta \subseteq \mathbb{R}^{r}$ in pripadajoč vektorski parameter $\theta = (\theta_{1},\ldots,\theta_{r}).$
Privzemimo, da imajo vse proučevane porazdelitve gostote oziroma verjetnostne funkcije oblike
\[
    f(x;\theta) = f(x;\theta_{1},\ldots,\theta_{r}).
\]
Funkcijo verjetja za vzorec velikosti $n$ definiramo kot funkcijo paramtetra $\theta$, in sicer
\[
    F(x_{1},\ldots,x_{n};\underbrace{\theta_{1},\ldots,\theta_{r}}_{\theta}) = f_{1}(x_{1},\theta)\cdots f_{n}(x_{n},\theta).
\]
Kot funkcija vektorja $x$ pa je $F$ gostota slučajnega vektorja $X = (X_{1},\ldots,X_{n}).$

Najti želimo tak parameter, v katerem bo funkcija verjetja zavzela svoj maksimum, torej 
\[
    F(\hat{\theta}) = \underset{\theta \in \overline{\Theta}}{\max}{F(\theta)}.
\]
Opazimo, da si računanje lahko precej poenostavimo, če obe strani zgornje enačbe logaritmiramo
\begin{equation}
    \log(F(\theta)) = L(\theta) = \sum_{i=1}^{n}\log f_{i}(x_{i},\theta).
\end{equation}
Funkciji $L$ rečemo logaritemska funkcija verjetja, njene stacionarne točke pa bodo kandidati za cenilko največjega verjetja. Ker je logaritem 
naraščajoča funkcija, bodo ekstremi $L$ in $F$ sovpadali. Rešiti moramo torej sistem enačb
\begin{equation}\label{mle}
    \frac{\partial}{\partial \theta_{j}}(L(\theta)) = 0~,j=1,\ldots,r,
\end{equation}
ki mu rečemo tudi sistem \textit{enačb verjetja}, odvod logaritemske funkcije verjetja pa v statistiki pogosto poimenujejo \textit{zbirna funkcija}.
Ko rešimo enačbe verjetja, najdemo ekstrem funkcije verjetja in dobimo \textit{cenilko največjega verjetja}, v angleščini pogosto označeno MLE (okrajšava za
\textit{maximum likelihood estimator}).

Tako dobljene cenilke niso nujno nepristranske, so pa dosledne, če je rešitev \eqref{mle} enolična. V splošnem take enačbe niso rešljive eksplicitno, zato se poslužujemo različnih
numeričnih metod za njihovo reševanje. Nekatere so predstavljene v drugem delu naloge.

\subsection{Linearna regresija} %DOPOLNI NORMALNI SISTEM ENAČB
Linearna regresija je najenostavnejši primer posplošenega linearnega modela. Enostavno jo lahko zapišemo kot:
$
    Y = X \beta + \varepsilon
$
kjer je $Y$ proučevan slučajni vektor dimenzije n, $X \in \mathbb{R}^{n\times (p+1)} $ je matrika pojasnjevalnih slučajnih spremenljivk, $\beta$ je vektor koeficientov dimenzije $p+1$, 
ki jih želimo oceniti, $\varepsilon$ pa slučajna spremenljivka, ki predstavlja napako - pri računanju, meritvah \ldots. Privzemimo, da je $E(\varepsilon) = 0$. Iz tega sledi
$\mu = E(Y) = X\beta$. Model torej pričakovano vrednost slučajne spremenljivke predstavi kot linearno funkcijo pojasnjevalnih spremenljivk.
Parametre $\beta$ ocenimo z metodo najmanjših kvadratov - iščemo tak $\hat{\beta}, $ ki bo zadoščal
\[
    \lVert y - X\hat{\beta} \rVert^2 = \underset{\beta}{min}\lVert y-X\beta\rVert^2.
\]
Želimo torej element slike matrike X, ki bo v drugi normi najbližje vektorju $y, $ in izkaže se, da je to ravno pravokotna projekcija tega vektorja na $im(X).$ To lahko zapišemo Kot
\[
    \langle y-X\hat{\beta}, Xh \rangle = 0~\forall h\in\mathbb{R}^{p+1},
\]
kar pa velja natanko tedaj kot
\begin{align*}
    X^\top(y-X\hat{\beta}) &= 0 \\
    (X^\top X)\hat{\beta} &= X^\top y.
\end{align*}
Če je $rang(X) = p+1$ in je posledično matrika $X^\top X$ obrnljiva, dobimo enolično rešitev po metodi najmanjših kvadratov oblike 
\[
    \hat{\beta} =  (X^\top X)^{-1}X^\top y.
\]

\subsection{Logistična regresija}
Logistična regresija se uporablja za določanje deležev oziroma računanje verjetnosti. V poštev pride, ko imamo odgovore tipa uspeh-neuspeh oziroma
govorimo o prisotnosti ali odsotnosti neke lastnosti. Kot smo že omenili, bomo proučevali vektor, katerega komponente so iz eksponentne družine z disperzijskim parametrom,
kamor seveda spada tudi binomska porazdelitev: $Y_{i} \sim B(n_{i}, p_{i})$. Ta pravi, da je 
\[
    P(Y_{i} = y_{i}) = {n_{i} \choose y_{i}} p_{i}^{y_{i}}(1 - p_{i}) ^{n_{i} - y_{i}}
\]
Pričakovana vrednost in varianca sta odvisni le od $p_{i}$, in sta enaki $E(Y_{i}) = n_{i}p_{i} \text{~in}\\Var(Y_{i}) = n_{i}p_{i}(1 - p_{i}).$~
Poglejmo si sedaj podrobneje \textit{logit} transformacijo. Če se spomnemo, želimo določiti verjetnost nekega dogodka pri danih podatkih. Ob uporabi
identitente transformacije se nam kaj hitro lahko zgodi, da za posamezne verjetnosti dobimo vrednosti izven intervala $[0,1]$. Ta problem bomo rešili v 
dveh korakih.
Najprej uvedimo 
\[ 
    \mathrm{obeti}_{i} = \frac{p_{i}}{1 - p_{i}} %poglej kako se tle da vejico
\]
kjer se premaknemo iz verjetnosti v \textit{deleže} -- verjetnost dogodka proti verjetnosti, da se ne bo zgodil. Če je $p_{i}$ enak $\frac{1}{2}$, 
bo delež enak 1. Vidimo, da so deleži vedno pozitivni in niso omejeni navzgor.
V naslednjem koraku pa poglejmo logaritem deležev ali logit verjetnosti
\[
    \eta_{i} = \mathrm{logit}(p_{i}) = \log \frac{p_{i}}{1 - p_{i}}
\]
s tem pa si odstranimo tudi omejitev navzdol. Opazimo še, da če je $p_{i} = \frac{1}{2}$, je delež enak 1 in je logaritem 0. Kot funkcija p, je logit
strogo naraščajoča, torej imamo inverz. Označimo z $\eta_{i} = \exp{x_{i}^\top\beta}. $Običajno ga imenujemo \textit{antilogit}, izrazimo ga z:
\[
    p_{i} = \mathrm{logit}^{-1}(\eta_{i}) = \frac{\exp{\eta_{i}}}{1+\exp{\eta_{i}}}
\]
Vse skupaj nam da \textit{logistični model}, ki za slučajni del vzame binomsko porazdelitev. %VPRAŠAJ KAJ JE Z NAPAKO
Kot vidimo, zveza med prediktorji in verjetnostjo ni linearna, zato je težko oceniti, kako bo sprememba parametrov
vplivala na verjetnost. Na to vprašanje lahko približno odgovorimo tako, da odvajamo po spremenljivki $x_{j}$ (kar ima seveda smisel le za zvezne
pojasnjevalne spremenljivke) in dobimo $\nicefrac{\partial}{\partial x_{j}} = \beta_{j}p_{i}(1 - p_{i}).$~Vidimo, da na spremembo j-tega prediktorja
vpliva tako verjetnost kot tudi parameter $\beta$.

\subsubsection{Ocenjevanje parametrov}\label{ocenpar}
Imamo binomske slučajne spremenljivke in imamo povezovalno funkcijo, $\mathrm{logit}(p_{i}) = X\beta$, kjer so $\beta$ neznani parametri.
V naslednjem razdelku si bomo ogledali kako zanje izpeljemo enačbe verjetja, ki jih nato uporabimo v numeričnih algoritmih.
Kot v vsakem posplošenem linearnem modelu tudi v tem predpostavimo neodvisnost komponent slučajnega vektorja $\mathrm{Y}$ zato 
\begin{align*}
    P(Y = \vec{y}) &= \prod_{i=1}^{n} P(Y_{i} = y_{i}) \\
                    &=\prod_{i=1}^{n} {n_{i} \choose y_{i}} p_{i}^{y_{i}}(1 - p_{i})^{n_{i} - y_{i}}
\end{align*}
Naprej si oglejmo logaritemsko funkcijo verjetja. V nadaljnem računanju bom izpuščal binomski simbol na začetku - je samo konstanta, ki na
končen rezultat nima vpliva. Po prejšnjih oznakah je torej
\begin{align}\label{logit1}
    L(p_{i}) &\propto \log\{\prod_{i=1}^{n} p_{i}^{y_{i}}(1 - p_{i})^{n_{i} - y_{i}} \} \nonumber  \\
        &\propto \sum_{i=1}^{n}\{y_{i}\log{p_{i}} + (n_{i} - y_{i})\log(1 - p_{i})\} \nonumber \\
        &\propto \sum_{i=1}^{n}\{n_{i}\log{(1-p_{i})}  + y_{i}\log{\left(\frac{p_{i}}{1-p_{i}}\right)}\}
\end{align}
Po predpostavki logističnega modela je
\[
   \mathrm{logit}(p_{i}) = \log\left( \frac{p_{i}}{1-p_{i}}  \right) = x_{i0}\beta_{0} + x_{i1}\beta_{1} + \ldots + x_{ir}\beta_{r} = x_{i}^\top \beta,
\]
kjer je $x_{i0}=1,~i=1,\ldots,n$

Od tod lahko izrazimo verjetnosti $p_{i}$
\begin{align}
    p_{i} &= \frac{\exp{x_{i}^{\top} \beta}}{1 + \exp{x_{i}^\top\beta}} \text{~ter} \\
    1 - p_{i} &= \frac{1}{1 + \exp{x_{i}^\top\beta}}.
\end{align}

Spodnji funkciji rečemo \textit{sigmoida}, definiramo jo kot 
\[
    f(x) = \frac{e^x}{1+e^x}.
\] Iz njenega grafa je morda še bolj očitno, zakaj jo je smiselno uporabiti za modeliranje verjetnosti
\begin{center}
\begin{figure}[h]
\begin{tikzpicture}
    \begin{axis}[
        axis lines = center,
        %xtick = {-10,-9,-8,...,9,10}
        ytick = {0,0.1,0.2,...,0.9,1},
        ylabel = verjetnost,
        y label style = {at={(axis description cs:0.25,.5)},rotate=90}%,anchor=south}
    ]
    \addplot [
        domain=-8:8, 
        samples=100, 
        color=black,
        ]
        {exp(x)/(1+exp(x))};
    %\addlegendentry{$x^2 + 2x + 1$}
    
    \end{axis}
\end{tikzpicture}
\caption{Graf sigmoide}
\label{fig:sigmoid}
\end{figure}
\end{center}

Če izpeljane izraze za verjetnost upoštevamo v logaritemski funkciji verjetja dobimo
\begin{align}
    L(\beta) &\propto \sum_{i=1}^{n}\left( n_{i}\log{\frac{1}{1 + \exp{x_{i}^\top\beta}}}  + y_{i}\log{\left( \frac{\frac{\exp{x_{i}^{\top} \beta}}{1 + \exp{x_{i}^\top\beta}}}{\frac{1}{1 + \exp{x_{i}^\top\beta}}}  \right)} \right) \nonumber\\
                &\propto \sum_{i=1}^{n}\left( y_{i}(x_{i}^\top\beta) - n_{i}\log(1 + \exp{x_{i}^\top\beta})\right)
\end{align}
Od tod vidimo, da je naša funkcija verjetja zares odvisna le od parametrov $\beta$,~vse ostalo nam je poznano. Da torej poiščemo maksimum in s tem
cenilko največjega verjetja, funkcijo odvajamo in zbirno funkcijo enačimo z 0
\[
    \frac{\partial}{\partial \beta} L = \begin{bmatrix}
                                 \frac{\partial L(\beta)}{\partial \beta_{0}} \\
                                 \frac{\partial L(\beta)}{\partial \beta_{1}} \\
                                 \vdots \\
                                 \frac{\partial L(\beta)}{\partial \beta_{p}}
                        \end{bmatrix}
\]
Pomembno je opaziti, da parametri $\beta$ vedno nastopajo ob pojasnjevalnih spremenljivkah linearno, zato bodo vse komponente enake oblike.
J-ta komponenta bo tako enaka
\begin{align}
    \frac{\partial L(\beta)}{\partial \beta_{j}} &= \sum_{i=1}^{n} \left(x_{ij}(y_{i} - n_{i}p_{i}(\beta))   \right),~~j = 0,1,\ldots r,~~\text{kjer~smo~upoštevali} \\
    \frac{\partial}{\partial \beta_{j}}(x_{i}^\top\beta) &= \frac{\partial}{\partial \beta_{j}}\left(\beta_{0} + x_{i1}\beta_{1} + \ldots x_{ir}\beta_{r}\right) \nonumber\\
                                                    &= x_{ij},
\end{align}
ter
\begin{align}                                         
    \frac{\partial}{\partial \beta_{j}} \log(1 + \exp(x_{i}^\top\beta)) &= \frac{ \frac{\partial}{\partial \beta_{j}}\exp(x_{i}^\top\beta) }{1 + \exp(x_{i}^\top\beta)} \nonumber \\
    &= \frac{\exp(x_{i}^\top\beta)}{1 + \exp(x_{i}^\top\beta)} \frac{\partial}{\partial \beta_{j}} (x_{i}^\top\beta) \nonumber \\
    &=p_{i}(\beta)x_{ij}
\end{align}
Enačbe, ki jih s tem postopkom dobimo, v splošnem niso eksplicitno rešljive. Za reševanje se uporablja numerične metode, ki slonijo na Newtonovi iteraciji. Kot
bomo kasneje pokazali, je zanjo potrebno izračunati še drugi odvod, zato to storimo tu. Zopet odvajamo po komponentah, tako kot zgoraj. Najprej izračunajmo
\begin{align}
    \frac{\partial p_{i}(\beta)}{\partial \beta_{k}} &= \frac{\partial}{\partial \beta_{k}} \frac{\exp{x_{i}^\top\beta}}{1+\exp{x_{i}^\top\beta}} \nonumber \\
        &= x_{ik}p_{i}(\beta)(1 - p_{i}(\beta)) \nonumber
\end{align}
Vse sedaj skupaj sestavimo v
\begin{align}
    \frac{\partial^2}{\partial \beta_{j}\partial\beta_{k}} L(\beta)= - \sum_{i}^{n}\left(x_{ij}x_{ik}n_{i}p_{i}(\beta)(1-p_{i}(\beta))\right),~~j,k = 0,1,\ldots, r
\end{align}
Spomnimo se, da delamo z binomskimi slučajnimi spremenljivkami in torej velja $var(Y_{i}) = v_{i}(\beta) = n_{i}p_{i}(1-p_{i}),$~kar vključimo v zgornjo enačbo in končno dobimo
\begin{align}
    \ddot{\ell}(\beta) = -\sum_{i=1}^{n}\left(x_{ij}x_{ik}v_{i}(\beta)\right).
\end{align}
Zapišimo zgoraj izpeljane zveze v berljivejšo matrično notacijo. 
\begin{equation*}
    \log \left(\frac{p}{1-p}\right) = \mathbf{X}\beta
\end{equation*}
Vektorsko definiramu tudi
\[
    \exp{\mathbf{X}\beta} = \begin{bmatrix}
                            \exp{x_{1}^\top\beta} \\
                            \vdots\\
                            \exp{x_{n}^\top\beta}
                            \end{bmatrix},
\]
spomnimo se enačbe \eqref{logit1} in iz nje izpeljimo 
\begin{align} \label{ell}
    L(\beta) &= \sum_{i=1}^{n}\{n_{i}\log{1-p_{i}}  + y_{i}\log{\left(\frac{p_{i}}{1-p_{i}}\right)}\} \nonumber\\
    &= y^\top\mathbf{X}\beta - n^\top \log(1 + \exp{\mathbf{X}\beta}),
\end{align}
in še odvoda zgornje funkcjie, ki pa ga lahko zapišemo kot
\begin{equation}
    \dot{L}(\beta) = \mathbf{X}^\top(y - m\circ p(\beta)),
\end{equation}
kjer je s $\circ$ označeno Hadamardovo množenje po elementih.
S pričakovano vrednostjo vektorja označimo vektor pričakovanih vrednosti komponent in torej lahko zapišemo
\begin{equation}
    \mathrm{E}(Y) = m \circ p(\beta) \equiv \mu(\beta),
\end{equation}
in lahko končno vse povzamemo v
\begin{align}\label{prvi}
    \dot{L}(\beta) = \mathbf{X}^\top(y - m \circ p(\beta)) = X^\top(y - \mu(\beta))
\end{align}
Ostane nam le še dvojni odvod. Najprej si oglejmo
\[
    v(\beta) = \begin{bmatrix}
        v_{1}(\beta)  & & &\\
        & v_{2}(\beta) & & \\
        & & \ddots & \\
        & & & v_{n}(\beta)
    \end{bmatrix},
\]
iz tega potem takoj sledi, da je
\begin{equation} \label{drugi}
    \ddot{L}(\beta) = -\mathbf{X}^\top v(\beta)\mathbf{X},
\end{equation}
torej element v $j$-ti vrstici in $k$-tem stolpcu je $\sum_{i=1}^{n}x_{ij}x_{ik}v_{i}(\beta).$

\subsection{Obstoj rešitve enačb verjetja v logističnem modelu}
\textit{To poglavje je še nepopolno, moram razmisliti kaj vključiti.}

V prejšnjem odseku smo izpeljali enačbe verjetja za logistično regresijo in videli, da v splošnem niso analitično rešljive. Porodi pa se vprašanje,
kdaj rešitev pravzaprav sploh obstaja? Izkaže se, da je obstoj in enoličnost rešitve v logističnem modelu moč dokazati iz podatkov. Sledeče poglavje temelji na članku
\cite{albert1984existence}.

Zopet bomo vpeljali nekaj novih oznak. Imejmo $n$ neodvisnih opazovanj vektorja dimenzije $p$ in določimo $(x,H),$ kjer je $x^\top = (x_{0},\ldots,x_{p}),x_{0}\equiv 1,$
$H$ pa je spremenljivka, ki zavzame vrednosti $H_{1},\ldots,H_{g}$ in pokaže kateri skupini pripada določeno opazovanje. V našem primeru je $g=2$, $H_{1}$ ustreza Bernoullijevi
enki, $H_{2}$ pa Bernoullijevi ničli.

\subsubsection{Funkcija verjetja}
Za potrebe tega poglavja vpeljimo novo notacijo za funkcijo verjetja
\begin{equation}
    \begin{aligned}
    \mathbf{pr}(H_{s}|x) = \exp(\beta_{s}^\top x)\mathbf{pr}(H_{g}|x),~s=1,\ldots,g-1 \\
    \mathbf{pr}(H_{g}|x) = \nicefrac{1}{\sum_{s=1}^{g}\exp(\beta_{s}^\top x)} \\
    \beta_{s}^\top = (\beta_{s0},\ldots,\beta_{sp}),~s=1,\ldots,g-1,~\beta_{g}^\top=0
    \end{aligned}
\end{equation}
Za ilustracijo si zopet poglejmo primer $g=2,$ enačbe verjetja potem izgledajo
\begin{equation}
    \begin{aligned}
    \mathbf{pr}(H_{2}|x) = \frac{1}{\exp(\beta_{1}^\top x) + \exp{(\beta_{2}^\top x)}} = \frac{1}{1 + \exp(\beta^\top x)} \\
    \mathbf{pr}(H_{1}|x) = \frac{\exp(\beta_{1}^\top x)}{1 + \exp(\beta_{1}^\top x)},\\
    \end{aligned}
\end{equation}
kar se seveda sklada z enačbami iz prejšnjega poglavja.

Potrebujemo še način za razvrščanje vektorjev v skupine. Vektor $x$ pripada skupini $H_{s}$ natanko tedaj, ko velja
\[
    (\beta_{s}-\beta_{t})^\top x \geq 0,~t=1,\ldots,g.
\]
Predpostavimo še, da je matrika opazovanj $X$, dimenzije $n\times(p+1),$ polnega ranga. Označimo z $E_{s}$ množico identifikatorjev vrstic matrike $X,$ ki pripadajo
skupini $H_{s}.$
Logaritemsko funkcijo verjetja v splošnem zapišemo kot
\[
    \log L (X,\beta) = \sum_{j=1}^{g}\sum_{i\in E_{j}}\log\left(\frac{1}{\sum_{t=1}^{g} \exp(\beta_{t}-\beta_{j})^\top x_{i}}\right)
\]

V nadaljevanju bomo ločeno obravnavali možnosti, kako so lahko podatki razporejeni glede na to, kateri skupini pripadajo. Podrobneje si bomo pogledali in narisali
primere, ko sta skupini dve - tedaj imamo tri možnosti:
\begin{itemize}
    \item podatki so popolnoma ločeni - popolna separacija
    \item podatki so popolnoma ločeni, vendar nekateri ležijo ravno na meji - nepopolna separacija
    \item podatki se prekrivajo - prekrivanje
\end{itemize}
Na podlagi tega lahko določimo, ali rešitev enačb verjetja obstaja in je enolična.

\subsubsection{Popolna separacija}
Popolna separacija v podatkih je prisotna, če obstaja tak vektor $\beta$, da za vse $i\in E_{j}$ in $j,t = 1,\ldots,g,~j\neq t$ velja
\[
    (\beta_{j}-\beta_{k})^\top x_{i} > 0.
\]
Torej obstaja vektor $\beta,$ ki nam podatke popolnoma loči na skupine. 
V primeru dveh skupin se pogoj prevede na $\beta^\top x_{i}>0, i\in E_{1}$ in $\beta^\top x_{i}<0, i\in E_{2}.$


\begin{izrek}
    Če je v podatkih prisotna popolna separacija, cenilka največjega verjetja $\hat{\beta}$ ne obstaja in velja
    \[
        \max_{\beta} L(X,\beta) = 1.
    \]
\end{izrek}

Izkaže se, da svoj maksimum funkcija verjetja doseže, ko parameter pošljemo v neskončnost - torej končna rešitev in s tem cenilka največjega verjetja ne obstaja. Grafično
si v primeru $g = 2$ popolno separacijo predstavljamo takole
\begin{center}
    \begin{figure}[h!]
    \begin{tikzpicture}
    \draw (0,4) -- (0,0) -- (4,0);
    \draw (3.75,0.25) -- (0.25,3.75);
    \filldraw[black] (2.2,0.8) circle (2pt);
    \filldraw[black] (1,2.75) circle (2pt);
    \filldraw[black] (2,1.7) circle (2pt);
    \filldraw[black] (0.5,1.45) circle (2pt);
    \filldraw[black] (1,0.5) circle (2pt);
    \filldraw[black] (3,0.25) circle (2pt);
    
    \filldraw[red] (3,1.25) circle (2pt);
    \filldraw[red] (2,3.4) circle (2pt);
    \filldraw[red] (1.6,3.2) circle (2pt);
    \filldraw[red] (3.5,3.5) circle (2pt);
    \filldraw[red] (0.6,3.9) circle (2pt);
    \filldraw[red] (2.5,2.5) circle (2pt);
    
    \end{tikzpicture}
    \caption{Popolna separacija}
\end{figure}
\end{center}
kjer ena barva predstavlja podatke v prvi skupini, druga pa v drugi. Podatke lahko s premico razdelimo na dva dela, v katerem so samo tisti, ki ustrezajo bodisi Bernoullijevi enici
bodisi Bernoullijevi ničli.

\subsubsection{Nepopolna separacija}
Nepopolna separacija v podatkih je prisotna, če obstaja tak vektor $\beta$, da za vse $i\in E_{j}$ in $j,t = 1,\ldots,g,~j\neq t$ velja
\[
    (\beta_{j}-\beta_{k})^\top x_{i} \geq 0,
\]
pri čemer velja enakost za vsaj eno trojico $(i,j,t).$

Kot je omenjeno že zgoraj, v tem primeru podatke lahko popolnoma ločimo, a nekatri ležijo popolnoma na meji. Lahko pa se zgodi, da je podatke možno ločiti na več načinov. V spodnjih
slikah sta obravnavana oba primera - ko imamo eno samo ločnico in ko jih je možno najti več.
\begin{center}
    \begin{figure}[h!]
    \begin{tikzpicture}
    \draw (0,4) -- (0,0) -- (4,0);
    \draw (3.75,0.25) -- (0.25,3.75);
    \filldraw[black] (2.2,0.8) circle (2pt);
    \filldraw[black] (1,3) circle (2pt);
    \filldraw[black] (2,2) circle (2pt);
    \filldraw[black] (0.5,1.45) circle (2pt);
    \filldraw[black] (1,0.5) circle (2pt);
    \filldraw[black] (3,0.25) circle (2pt);
    
    \filldraw[red] (3,1) circle (2pt);
    \filldraw[red] (2,3.4) circle (2pt);
    \filldraw[red] (1.6,3.2) circle (2pt);
    \filldraw[red] (3.5,3.5) circle (2pt);
    \filldraw[red] (0.6,3.9) circle (2pt);
    \filldraw[red] (2.5,2.5) circle (2pt);
    
    \end{tikzpicture}
    \caption{Nepopolna separacija, prvi primer}
\end{figure}
\end{center}
%MANJKA PRIMER KO IMAŠ VEČ LOČNIC


\subsubsection{Prekrivanje}
Če je v podatkih prisotno prekrivanje, torej ne padejo v nobeno od prejšnjih dveh kategorij, je sistem enačb verjetja rešljiv enolično.
\begin{center}
    \begin{figure}[h!]
    \begin{tikzpicture}
    \draw (0,4) -- (0,0) -- (4,0);
    \draw (3.75,0.25) -- (0.25,3.75);
    \filldraw[black] (2.2,0.8) circle (2pt);
    \filldraw[red] (1,2.75) circle (2pt);
    \filldraw[black] (2,1.7) circle (2pt);
    \filldraw[black] (0.5,1.45) circle (2pt);
    \filldraw[red] (1,0.5) circle (2pt);
    \filldraw[red] (3,0.25) circle (2pt);
    
    \filldraw[black] (3,1.25) circle (2pt);
    \filldraw[black] (2,3.4) circle (2pt);
    \filldraw[red] (1.6,3.2) circle (2pt);
    \filldraw[black] (3.5,3.5) circle (2pt);
    \filldraw[red] (0.6,3.9) circle (2pt);
    \filldraw[red] (2.5,2.5) circle (2pt);
    
    \end{tikzpicture}
    \caption{Prekrivanje}
\end{figure}
\end{center}
Vidimo, da v tem primeru podatkov ni možno razdeliti na dve ločeni skupini


\subsubsection{Ugotavljanje separacije}


\subsection{Kanonični modeli v splošnem}
Kot bomo spoznali v sledečem razdelku, spada logistična regresija med tako imenovane modele s "'kanonično"` povezovalno funkcijo. 
Za vpeljavo tega ter prenekaterih ostalih pojmov pa potrebujemo nekaj dodatne teorije.

\subsubsection{Pomembnost kanoničnih povezvalnih funkcij}\label{kan}
Kot smo omenili že v uvodu povezovalna funkcija opisuje relacijo med pričakovano vrednostjo opazovane spremenljivke in desno stranjo našega modela, torej
sistematično komponento modela. Eksponentno družino torej v splošnem sestavljajo porazdelitve, z gostotami oblike
\[
    f_{Y}(y; \theta, \phi) = \exp{\left(\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)\right)}.
\]
Enačbo logaritmiramo in dobimo
\[
    L(y;\theta) = \frac{y\theta-b(\theta)}{a(\phi)} + c(y;\phi),
\]
njen odvod, torej funkcija zbira pa je 
\[
    \frac{\partial}{\partial \theta} L(y;\theta) = \frac{y-b'(\theta)}{a(\phi)}.
\]
O kanonični povezovalni funkciji govorimo, če velja $\theta=\eta,$ torej je naravni parameter eksponentne družine ravno enak funkciji pričakovane vrednosti v modelu.
Da uporabimo logit verjetnosti v logističnem modelu torej ni naključje - videli smo, da je $\textbf{logit}(p_{i})$ enak parametru $\theta.$ Spodaj je navedenih še nekaj ostalih
kanoničnih povezovalnih funkcij, njihovo uporabnost bomo spoznali v naslednjem razdelku.

\begin{center}
    \begin{tabular}{ | c | c | c |}
        \hline
        Porazdelitev & $f(\mu)$ & Uporaba \\
        \hline
        Normalna & $id(\mu)$ &  Linearni odgovori\\
        Poissonova & $\log \mu$ & Število pojavitev \\
        Binomska & $\mathrm{logit}\mu$ & Binarni podatki\\
        Gamma & $-\frac{1}{\mu}$ & \\
        \hline
    \end{tabular}
\end{center}

Kot smo že v zgledu z logistično regresijo videli, potrebujemo odvode logaritemske funkcije verjetja po parametrih $\beta.$ Uporabiti moramo torej verižno pravilo 
\[
    \frac{\partial L}{\partial\beta_{j}} = \left(\frac{\partial L}{\partial \theta}\right)\left(\frac{\partial \theta}{\partial\mu}\right)
    \left(\frac{\partial\mu}{\partial\eta}\right)\left(\frac{\partial\eta}{\partial\beta_{j}}\right).
\]
Lotimo se ga po korakih. Prvi člen smo že zgoraj izračunali kot $\frac{y-b'(\theta)}{a(\phi)}.$ Z uporabo $(b')^{-1}(\mu) = \theta$ in pravila za odvajanje inverzne funkcije 
dobimo $\frac{\partial \theta}{\partial\mu} = \frac{1}{b''(\theta)} = \frac{a(\phi)}{\mathrm{Var}(Y)},$ zadnji člen pa bo kar vedno enak $x_{ij}.$ Tretji člen je odvisen
od povezovalne funkcije in se mu bomo posvetili nekoliko kasneje. Sestavimo vse skupaj in dobimo
\[
    \frac{\partial L}{\partial\beta_{j}} = \frac{y-\mu}{var(Y)}\frac{\partial \mu}{\partial\eta}x_{ij}.
\]

Opazimo: če imamo opravka s kanonično povezovalno funkcijo je $\eta = \theta!$ Torej namesto odvajanja po prvem parametru, lahko $\mu$ odvajamo po $\theta$ in
dobimo $\frac{\partial\mu}{\partial\theta} = b''(\theta)$ in se odvod še dodatno poenostavi v
\begin{equation}
    \frac{\partial L}{\partial\beta_{j}} = \frac{y-\mu}{var(Y)}b''(\theta)x_{ij} = \frac{y-\mu}{a(\phi)}x_{ij}.
\end{equation}

Za numerične metode bomo potrebovali še druge odvode, kjer pa nam pomaga informacijska enakost iz dokaza trditve \ref{izp}.

Najprej izračunajmo pričakovano vrednost odvoda funkcije zbira
\begin{align*}
    -\mathbb{E}(\frac{\partial^2 L}{\partial\beta_{j}\partial\beta_{k}}) &= \mathbb{E}((\frac{\partial L}{\partial \beta_{j}})(\frac{\partial L}{\partial \beta_{k}})) \\
    &= \mathbb{E}(\frac{y-\mu}{var(Y)^2})(\frac{\partial\mu}{\partial\eta})^{2}x_{ij}x_{ik} \\
    &= \frac{1}{var(Y)}(\frac{\partial\eta}{\partial\mu})^{2}x_{ij}x_{ik} \\
    &= \frac{b''(\theta)}{a(\phi)}x_{ij}x_{ik}.
\end{align*}

Po drugi strani pa je običajen drugi odvod enak
\begin{align*}    
    \frac{\partial^2L}{\partial\beta_{j}\partial\beta_{k}} &= \frac{\partial}{\beta_{k}}\{\left(\frac{\partial L}{\partial\theta}\right)\left(\frac{\partial \theta}{\partial\beta_{j}}\right)\}\\
    &= \frac{\partial L}{\partial\theta}\left(\frac{\partial^2\theta}{\partial\beta_{j}\partial\beta_{k}}\right) + \left(\frac{\partial\theta}{\partial\beta_{j}}\right)\left(\frac{\partial^2L}{\partial\theta^2}\frac{\partial\theta}{\partial\beta_{k}}\right) \\
    &= 0 + \frac{\partial^2L}{\partial\theta^2}x_{ij}x_{ik},
\end{align*}
prej pa smo že dokazali da je
\[
    \frac{\partial^2L}{\partial\theta^2} = -\frac{b''(\theta)}{a(\phi)}.
\]
Sledi
\begin{equation}\label{pv2odv}    
    \mathbb{E}\left(\frac{\partial^2L}{\partial\theta^2}\right) = \frac{\partial^2L}{\partial\theta^2}.
\end{equation}

Uporabnost zgornjega rezultata pa nam bo dalo poglavje o numeričnih metodah.

\subsubsection{Poljubna povezovalna funkcija}
Za poljubno povezovalno funkcijo smo v zgornjem razdelku pokazali
\begin{align*}    
    \frac{\partial}{\partial\beta_{j}}L &= \frac{y-\mu}{\mathrm{Var}(Y)}\left(\frac{\partial\mu}{\partial\eta}\right)x_{ij}\\
    -\mathbb{E}(\frac{\partial^2}{\partial\beta_{j}\partial\beta_{k}}) &= \frac{1}{\mathrm{Var}(Y)}\left(\frac{\partial\mu}{\partial\eta}\right)^2x_{ij}x_{ik}
\end{align*}


\subsection{Probit regresija} %a se bom s tem sploh ukvarjal tho
Probit regresija se uporablja v podobne namene kot logistična, torej za določanje verjetnosti in razvrščanje. Razvili so jo v tridesetih letih
dvajsetega stoletja, ime pa je skovanka -- pride iz angleških besed \textit{\textbf{prob}ability un\textbf{it}}. V glavnem se od logistične regresije
razlikuje v sistematičnem delu. Verjetnost pozitivnega izida torej po modelu predpostavljamo
\begin{equation}
    p_{i}(\beta) = \Phi (\beta_{0} + x_{i1}\beta_{1} + \ldots + x_{ir}\beta_{r}),
\end{equation}
kjer $\Phi$ predstavlja kumulativno porazdelitveno funkcijo standardne normalne slučajne spremenljivke. Ta seveda ni linearna (v nasprotju s prejšnjimi modeli),
podana je kot 
\[
    \Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{\frac{-t^2}{2}}\,dt
\]
Očitno v tem primeru ne delamo s kanonično povezovalno funkcijo, kot smo to počeli v prejšnjem poglavju.
\subsubsection{Ocenjevanje parametrov probit regresije}
Podobno kot v logističnem modelu, se bomo ocenjevanja parametrov lotili po metodi največjega verjetja. 

Za sistematični del modela privzemimo binomsko porazdeljene slučajne spremenljivke s parametroma $\mathrm{Bin}(m_{i},p_{i}(\beta))$,
verjetnost pozitivnega izida pa izrazimo z
\[
    P(Y_{i} = y_{i}) = \binom{m_{i}}{y_{i}}p_{i}(\beta)^{y_{i}}(1 - p_{i}(\beta))^{m_{i} - y_{i}} = \binom{m_{i}}{y_{i}} (\Phi(x_{i}^\top \beta)^{y_{i}})(1 - \Phi(x_{i}^\top \beta))^{m_{i} - y_{i}} 
\]
Funkcijo verjetja, tako kot zgoraj izrazimo z gostotami posameznih komponent
\[
    F(\beta) = \prod_{i=1}^{n} \binom{m_{i}}{y_{i}} \Phi(x_{i}^\top\beta)^{y_{i}}(1 - \Phi(x_{i}^\top\beta))^{m_{i} - y_{i}},
\]
kjer binomski simbol izpustimo zaradi enostavnejšega pisanja. Zgornjo enačbo logaritmiramo in dobimo
\begin{equation}
    \log(F(\beta)) = L(\beta) = \sum_{i = 1}^{n}\left(y_{i}\log\Phi(x_{i}^\top\beta) + (m_{i} - y_{i})\log(1 - \Phi(x_{i}^\top\beta)) \right)
\end{equation}
Enačbo odvajamo po parametru $\beta,$ vendar se nam v tem primeru ne poenostavi kot z logistično funkcijo. Označimo s $ \phi = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},$
gostoto standardne normalne porazdelitve.
\begin{align*}
    \frac{\partial}{\partial\beta_{j}} L(\beta) &=
    \frac{\partial}{\partial\beta_{j}} \sum_{i = 1}^{n}\left(y_{i}\log\Phi(x_{i}^\top\beta) + (m_{i} - y_{i})\log(1 - \Phi(x_{i}^\top\beta)) \right) = \\
    &= \sum_{i = 1}^{n}\left(\frac{\phi(x_{i}^\top\beta)}{\Phi(x_{i^\top\beta})}x_{ij} - (m_{i}-y_{i})\frac{\phi(x_{i}^\top\beta)}{1-\Phi(x_{i^\top\beta})} x_{ij}  \right) = \\
    &= \sum_{i = 1}^{n}\phi(x_{i}^\top\beta)x_{ij}\left(\frac{y_{i}}{\Phi(x_{i}^\top\beta)} - \frac{m_{i}-y_{i}}{1-\Phi(x_{i}^\top\beta)}\right) = \\
    &= \sum_{i = 1}^{n}\phi(x_{i}^\top\beta)x_{ij}\left(\frac{y_{i} - m_{i}\Phi(x_{i}^\top\beta)}{\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta))}\right)
\end{align*}
Sistem enačb verjetja se torej glasi
\begin{equation}
    \sum_{i = 1}^{n}\phi(x_{i}^\top\beta)x_{ij}\left(\frac{y_{i} - m_{i}\Phi(x_{i}^\top\beta)}{\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta))}\right) = 0,~~j=0,1,\ldots,r
\end{equation}
Te enačbe očitno niso rešljive analitično in se bomo zopet morali poslužiti numeričnih metod. Kot smo videli že pri izpeljavi enačb za logistično regresijo,
 bomo za to potrebovali še druge odvode.
\begin{multline*}
    \frac{\partial^2}{\partial\beta_{j}\partial\beta_{k}}L(\beta) = \sum_{i = 1}^{n}x_{ij}\left(\frac{\partial}{\partial\beta_{k}}[\phi(x_{i}^\top\beta)]\frac{y_{i} - m_{i}\Phi(x_{i}^\top\beta)}{\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta))}\right)\\
    +\frac{\partial}{\partial\beta_{k}}\left(\frac{y_{i} - m_{i}\Phi(x_{i}^\top\beta)}{\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta))}\right)\phi(x_{i}^\top\beta)
\end{multline*}
Izračunajmo najprej prvi člen
\begin{align*}
    \frac{\partial}{\partial\beta_{k}}\phi(x_{i}^\top\beta) &= \frac{\partial}{\partial\beta_{k}}\left(\frac{1}{\sqrt{2\pi}}e^{-\frac{(x_{i}^\top\beta)^2}{2}}\right) \\
    &=\frac{-x_{i}^\top\beta}{\sqrt{2\pi}}e^{-\frac{(x_{i}^\top\beta)^2}{2}}x_{ik} \\
    &= -x_{i}^\top\beta x_{ik}\phi(x_{i}^\top\beta).
\end{align*}
Drugi člen povzroča nekaj več preglavic.
\begin{align}\label{drugiodv}
    \frac{\partial}{\partial\beta_{k}}\left(\frac{y_{i} - m_{i}\Phi(x_{i}^\top\beta)}{\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta))}\right) &= 
    \frac{-m_{i}\phi(x_{i}^\top\beta)x_{ik}\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta)) - (y_{i}-m_{i})\nicefrac{\partial}{\partial\beta_{k}(\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta)))}}{(\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta)))^2}
\end{align}
Posebej izračunajmo še
\begin{align*}
    \frac{\partial}{\partial\beta_{k}}(\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta))) &= \phi_{x_{i}^\top\beta}x_{ik}(1-\Phi(x_{i}^\top\beta)) - \Phi(x_{i}^\top\beta)\phi(x_{i}^\top\beta)x_{ik}\\
    &= \phi(x_{i}^\top\beta)x_{ik}(1-2\Phi(x_{i}^\top\beta))
\end{align*}
in vključimo to v enačbo \eqref{drugiodv}
\begin{align*}
    &\frac{-m_{i}\phi(x_{i}^\top\beta)\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta))x_{ik} - (y_{i} - m_{i}\Phi(x_{i}^\top\beta))\phi(x_{i}^\top\beta)x_{ik}(1-2\Phi(x_{i}^\top\beta))}{(\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta)))^2} \\
    &\phi(x_{i}^\top\beta)x_{ik}\frac{2y_{i}\Phi(x_{i}^\top\beta) - m_{i}\Phi(x_{i}^\top\beta)^2 - y_{i}}{(\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta)))^2}
\end{align*}
Vse skupaj povzemimo v
\begin{align*} %NE VIDI SE VSEGA. POGTUNTAJ KAKO ZMANJŠAT FONT
    \frac{\partial^2}{\partial\beta_{j}\partial\beta_{k}}L(\beta) &= \sum_{i = 1}^{n}x_{ij}\left(-x_{i}^\top\beta x_{ik} \phi(x_{i}^\top\beta)\frac{y_{i}-m_{i}}{\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta))}
            +\phi(x_{i}^\top\beta)^2x_{ik}\frac{2y_{i} - m_{i}\Phi(x_{i}^\top\beta)^2 - y_{i}}{(\Phi(1-\Phi(x_{i}^\top\beta)))^2}\right) \\
    &= \sum_{i=1}^{n}x_{ij}\frac{\phi(x_{i}^\top\beta)}{\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta))}\left(\phi(x_{i}^\top\beta)\frac{2y_{i} - m_{i}\Phi(x_{i}^\top\beta)^2 - y_{i}}{(\Phi(1-\Phi(x_{i}^\top\beta)))}
    -(x_{i}^\top\beta)(y_{i}-m_{i}\Phi(x_{i}^\top\beta)) \right)x_{ik}
\end{align*}

Zopet je koristno enačbe zapisati v matrični obliki. Za funkcijo zbira definirajmo vektor faktorjev
\[
    s = \begin{bmatrix}
        \phi(x_{1}^\top\beta)\frac{y_{1}-m_{1}\Phi(x_{1}^\top\beta)}{\Phi(x_{1}^\top\beta)(1-\Phi(x_{1}^\top\beta))} \\
        \vdots \\
        \phi(x_{n}^\top\beta)\frac{y_{n}-m_{n}\Phi(x_{n}^\top\beta)}{\Phi(x_{n}^\top\beta)(1-\Phi(x_{n}^\top\beta))} \\
    \end{bmatrix}
\]
in tako funkcijo zbira poenostavimo v
\[
    \frac{\partial}{\partial\beta}L(\beta) = \mathbf{X}^\top s.
\]

Na podoben način se lotimo tudi Hessejeve matrike. Definiramo diagonalno matriko, kjer so na diagonali členi
\[
    h_{i} = \frac{\phi(x_{i}^\top\beta)}{\Phi(x_{i}^\top\beta)(1-\Phi(x_{i}^\top\beta))}\left(\phi(x_{i}^\top\beta)\frac{2y_{i} - m_{i}\Phi(x_{i}^\top\beta)^2 - y_{i}}{(\Phi(1-\Phi(x_{i}^\top\beta)))}
    -(x_{i}^\top\beta)(y_{i}-m_{i}\Phi(x_{i}^\top\beta)) \right),
\]
torej $\mathbf{H} = diag(h_{1},\ldots,h_{n})$ in Hessejevo matriko zapišemo v preglednejši obliki
\[
    \frac{\partial^2}{\partial\beta^2} L(\beta) = \mathbf{X}^\top \mathbf{H X}
\]
Končno, Newtonova metoda za iskanje ničel funkcije zbira z uporabo vseh zgornjih oznak
\begin{equation}
    \beta_{i+1} = \beta_{i} - (\mathbf{X}^\top \mathbf{H X})^{-1}\mathbf{X}^\top s.
\end{equation}


\section{Numerične metode}
V sledečih razdelkih si bomo od bliže pogledali nekaj numeričnih metod, uporabljenih v kasnejših zgledih. Te metode slonijo na stoletja starih
idejah, ki smo jih spoznali tekom študija, uporabljajo pa se tudi v številnih praktičnih aplikacijah.
\subsection{Newton -- Raphsonova metoda} \label{nr}
Newton -- Raphson (oziroma le Newtonova) metoda je bila v osnovi razvita za iskanje ničel funkcije. Spada v razred \textit{navadnih iteracij}, torej metod za iterativno
reševanje enačb $f(x) = 0,$ ki jih prevedemo na $g(x) = x,$ izberemo začetni približek $x_{0}$ in ponavljamo 
\[
    x_{r+1} = g(x_{r}).
\]

V najosnovnejši (ter najpogostejši) verziji za iskanje
ničle funkcije ene spremenljivke začnemo v neki točki, naslendnjo pa izberemo v presčišču tangente, izračunane v 
tej točki, z x-osjo. Postopek tako iterativno nadaljujemo. Ideja je torej sila preprosta, za izpeljavo pa tudi ni potrebno preveč truda.
Predpostavimo odvedljivost funkcije na nekem intervalu in recimo, da imamo trenuten približek $x_{n}.$~Razvijmo sedaj funkcijo v Taylorjev
polinom prve stopnje okoli $x_{n}:$
\[
    f(x) \approx f(x_{n}) + f'(x_{n})(x - x_{n})
\]
Presečišče najdemo, če zgornjo enačbo enačimo z 0 in dobimo znano formulo
\[
    x_{n+1} = x_{n} - \frac{f(x_{n})}{f'(x_{n})}.
\]
Metoda bo skonvergirala za začetne približke dovolj blizu ničli in v neki okolici ničle konvergirala s kvadratično hitrostjo. 
Na težave naletimo v več primerih. Najprej, blizu stacionarne točke metoda odpove, saj bi delili z 0 (oziroma vrednostmi blizu ničle, kar je numerično nestabilno).
Problem lahko predstavlja tudi računanje odvoda, ki zna biti zahtevno, ter dejstvo, da za slabe začetne približke ničle morda ne bomo našli. S temi 
težavami se bomo soočili v nadaljevanju.
Imamo torej algoritem, ki najde ničlo, v luči iskanja cenilke največjega verjetja pa bi želeli algoritem, ki poišče maksimum oziroma minimum funkcije.
Recimo, da imamo neko logaritemsko funkcijo verjetja $L$, in trenutni približek $\theta_{n}$. Razvijmo funkcijo okoli približka v Taylorjev polinom
druge stopnje:
\begin{align}
    L(\theta) \approx L(\theta_{n}) + \frac{\partial}{\partial\theta}L(\theta_{n})(\theta - \theta_{n}) + \frac{1}{2}(\theta - \theta_{n})^\top \frac{\partial^2}{\partial\theta^2}L(\theta_{n})(\theta - \theta_{n}) \label{taylor}
\end{align}
Maksimizirati želimo desno stran \eqref{taylor}. To storimo tako, da gradient $L$ enačimo z nič:
\[
    \frac{\partial}{\partial\theta} L(\theta_{n}) + \frac{\partial^2}{\partial\theta^2}L(\theta_{n})(\theta - \theta_{n}) = 0
\]
in izrazimo naslednji približek
\[
    \theta_{n + 1} = \theta_{n} - (\frac{\partial^2}{\partial\theta^2}L(\theta_{n}))^{-1}\frac{\partial}{\partial\theta}L(\theta_{n}).
\]
S tem postopkom imamo lahko dva problema. Prvič, lahko je zahtevno računati in invertirati drugi odvod (Hesian) funkcije, morda za kakšen $\theta_{n}$ sploh ne obstaja.
Drugič, proč od $\hat{\theta}$ lahko Newtonova metoda napreduje navzgor ali navzdol -- oboje je enako verjetno. Z drugimi besedami, Newtonova metoda ni naraščajoč algoritem in torej
ne da $L(\theta_{n}) < L(\theta_{n+1}).$ Mi pa bi želeli algoritem, ki bo konvergiral globalno (in ne le na nekem intervalu okoli rešitve).
Težavo z računanjem inverza rešimo tako, da namesto invertiranja problem prevedemo na reševanje sistema enačb za premik:
\begin{align}\label{newtoneq}
    x_{n + 1} &= x_{n} + p_{n} \nonumber\\
    \nabla^2L(\theta_{n})p_{n} &= -\nabla L(\theta_{n})
\end{align}
Zadnji vrstici v \eqref{newtoneq} rečemo tudi \textit{Newtonova enačba}.
Radi bi še dosegli, da bi se Newtonov algoritem premikal v eno smer, torej naraščal ali padal. S tem bi vedeli, kaj se bo zgodilo v iteraciji in lažje predvideli morebitne
nevšečnosti. Newtonova metoda za iskanje minimuma (maksimum) funkcije je optimizacijski problem drugega reda in realna funkcija ima globalni minimum (maksimum) tam, kjer je
njen drugi odvod pozitiven, oziroma v primeru funkcij več spremenljivk, kjer je njen Hesian pozitivno definiten (in je tam gradient enak nič). Če bi torej imeli
strogo pozivino definitno matriko, bi bil ta optimizacijski problem konveksen in kot tak rešljiv globalno (veljati morajo še pogoji Karush-Kuhn-Tuckerja, vendar je to skoraj vedno res).
Imejmo torej v točki $x^{*}$ pozitivno definitno Hessejevo matriko $H.$~Zapišimo Taylorjev polinom druge stopnje okoli te točke
\[
    f(x^{*} + s) = f(x_x^{*}) + \nabla f(x^{*})s + \frac{1}{2}s^\top H(x^{*})s.
\]
Če velja še pogoj prvega reda, torej $\nabla f(x^{*}) = 0,$~imamo
\[
    f(x^{*} + s) = f(x^{*}) + \frac{1}{2}s^\top H(x^{*})s,
\]
kar pomeni, da se vrednost funkcije vedno poveča, če se premaknemo iz stacionarne točke $x^{*}$~(drugi člen je vedno pozitiven zaradi pozitivne definitnosti)
Tako vidimo, da imamo strogo padajoč algoritem.

\subsubsection{Potencialne težave Newtonove metode}
Newtonova metoda ima mnogo pozitivnih plati, vendar pa je iz določenih vidikov precej občutljiva. Morda najbolj očiten problem je slaba izbira začetne točke iteracije. Če je ta
stacionarna točka obravnavane funkcije, nam metoda narekuje deljenje z 0, kar pa seveda nima smisla. Očiten primer bi bil iskanje ničle funkcije $f(x) = 1+x^2$ z začetnim približkom $x_{0} = 0.$
Na pamet takoj vidimo, da so ničle v $1$ in $-1$, če pa bi upoštevali iteracijo pa dobimo
\[
    x_{1} = x_{0} - \frac{f(x_{0})}{f'(x_{0})} = 0 - \frac{1}{0}.   
\]
Enaka težava seveda nastopi, če v sledečih korakih dobimo stacionarno točko oziroma se ji približujemo in tako delimo z vedno manjšimi števili, kar pa vodi v vselej slabše približke.

Sicer redkeje, ampak lahko se zgodi, da se približki "`zaciklajo"'. Primer take funkcije je $f(x) = x^3 - 2x + 2,$ če za začetni približek vzamemo 0. Tako v zaporednih korakih najprej dobimo
$x_{1} = 1$ in nato $x_{2} = 0$, kar pa je seveda naša začetna točka. Obstajajo okolice teh dveh točk, ki vedno konvergirajo v ta dvojni cikel in ne h iskani ničli. V splošnem 
zna biti obnašanje takega zaporedja precej zapleteno, imenuje se Newtonov fraktal in se vanj tu ne bomo spuščali.

Naslednja težava pa lahko nastopi, če se odvod naše funkcije lokalno ne obnaša dovolj "`lepo."' Prvič, odvod v ničli morda ne obstaja. Enostaven primer tega je 
$f(x) = \sqrt[3]{x}.$ Izračunamo lahko
\[
    x_{n+1} = x_{n} - \frac{x_{n} ^ {\nicefrac{1}{3}}}{\frac{1}{3}x_{n}^{1 - \nicefrac{1}{3}}} = -2x_{n},
\]
in vidimo, da za vsak začetni približek različen od nič metoda divergira. Splošneje, podoben rezultat dobimo za vsako funkcijo oblike $f(x) = \lvert x \rvert^{\alpha}, 0 < \alpha < \frac{1}{2},$
za $\alpha = \frac{1}{2}$ pa metoda sicer ne divergira, vendar se kot v prejšnjem primeru zacikla in ne pridemo do rešitve.

V zgoraj naštetih primerih torej Newtonova metoda ne konvergira h iskani ničli. Smiselno pa sledi vprašanje, katerim pogojem mora biti zadoščeno, da pa 
vednarle dobimo pravilno rešitev. To nam poda sledeči izrek:
\begin{izrek} 
    Naj iteracijska funkcija $g$ na intervalu $I = [ \alpha - \delta,\alpha + \delta ]$ zadošča Lipschitzovemu pogoju
    \[
        \lvert g(x) - g(y) \rvert \leq m \lvert x - y \rvert
    \]
    za poljubna $x,y \in I$ in konstanto 0 $\leq m \leq 1$. Potem za vsak $x_{0} \in I$ zaporedje $x_{r+1} = g(x_{r}), r \geq 1$ konvergira k $\alpha$. Poleg tega veljata
    tudi oceni
    \begin{equation}\label{prva_neenakost}
        \lvert x_{r} - \alpha \rvert \leq m^{r}\lvert x_{0} - \alpha \rvert
    \end{equation}
    in
    \begin{equation}
        \lvert x_{r+1} - \alpha \rvert \leq \frac{m}{1-m} \lvert x_{r} - x_{r-1} \rvert
    \end{equation}
\end{izrek}
\begin{proof}
    Označimo z $\varepsilon_{r} = x_{r} - \alpha $ napako približka $x_{r}.$ Velja
    \[
        \lvert \varepsilon_{r} \rvert = \lvert x_{r} - \alpha \rvert = \lvert g(x_{r-1}) - g(\alpha) \rvert \leq m\lvert x_{r-1} - \alpha \rvert = m \varepsilon_{r-1}.
    \]
    Ta postopek nadaljujemo in sledi
    \[
        \lvert \varepsilon_{r} \rvert \leq m \lvert \varepsilon_{r-1} \rvert \leq m^{2} \lvert \varepsilon_{r-2} \rvert \leq \ldots \leq m^{r} \lvert\varepsilon_{0} \rvert
    \]
    Od tu vidimo \eqref{prva_neenakost}, iz katere sledi da zaporedje $x_{r}$ konvergira proti $\alpha.$

    Za drugo neenakost pa si oglejmo
    \[
        \lvert x_{r+1} - \alpha \rvert \leq \lvert x_{r+1} - x_{r+2} \rvert + \lvert x_{r+2} - x_{r+3} \rvert + \ldots
    \]
    Upoštevamo še
    \[
        \lvert x_{r+k} - x_{r+k+1} \rvert = \lvert g(x_{r+k-1}) - g(x_{r+k}) \rvert \leq m \lvert x_{r+k-1} - x_{r+k} \rvert \leq \ldots \leq m^{k}v\lvert x_{r-1} - x_{r}\rvert 
    \]
    in končno dobimo
    \[
        \lvert x_{r+1} - \alpha\rvert \leq (m + m^2 + \cdots)\lvert x_{r-1} - x_{r}\rvert = \frac{m}{1-m}\lvert x_{r-1} - x_{r}\rvert
    \]
\end{proof}

Dodatno informacijo o območju konvergence navadne iteracije nam dajo tudi vrednosti odvoda iteracijske funkcije, o čemer govori naslednji izrek.
\begin{izrek}
    Naj bo iteracijska funkcija zvezno odvedljiva v negibni točki $\alpha$ in naj velja $\lvert g'(\alpha) \rvert < 1.$ Potem obstaja okolica I negibne točke,
    da za vsak začetni približek $x_{0} \in I$ iteracija konvergira k $\alpha.$
\end{izrek}
\begin{proof}
    Odvod je po predpostavki na neki okolici $\alpha$ strogo manjši od 1 in zaradi zveznosti obstajata $\delta > 0$ in konstanta $m < 1,$ da je $\lvert g'(x) \rvert < m < 1 za x \in I,$
    kjer z $I$ označimo $[\alpha - \delta,\alpha + \delta ].$ Potem po Lagrangeovem izreku velja $\lvert g(x) - g(y) \rvert \leq \lvert g'(\xi) \rvert \dot \lvert x - y \rvert,$
    za poljubna $x,y \in I.$ Ker je odvod na tem intervalu manjši od 1 sledi da je funkcija g Lipschitzova in zato po \eqref{prva_neenakost} konvergira.
\end{proof}

\subsubsection{Asimptotsko obnašanje in konvergenca}
Kot je pri numeričnih metodah to običajno, nas zanima njihovo obnašanje po več ponovitvah iteriacije. Pomembno je, kako hitro pridemo do rešitve saj želimo računanje čim manjkrat
ponoviti in dobiti najboljši možen rezultat.

Definirajmo si \textit{red konvergence}. Idejno je to število točnih decimalnih mest, ki jih pridobimo z vsakim korakom iteracije. 
\begin{definicija}\label{red_konv}
    Naj zaporedje $(x_{r})$ konvergira k $\alpha.$ Red konvergence je enak p, če obstajata taki števili $C_{1}, C_{2},$ da velja
    \[
        C_{1}\lvert x_{r} - \alpha \rvert ^{p} \leq \lvert x_{r+1} - \alpha \rvert \leq C_{2} \lvert x_{r} - \alpha \rvert^{p}.
    \]
    Ekvivalentno: red konvergence je p, če obstaja $C > 0$ tak, da
    \[
        \lim_{r\to\infty} \frac{\lvert x_{r+1} - \alpha\rvert}{\lvert x_{r} - \alpha\rvert^{p}} = C.
    \]
\end{definicija}

Red konvergence navadne iteracije je običajno precej enostavno določiti. Metodo nam daje naslenji izrek
\begin{izrek}
    Naj bo iteracijska funkcija g v okolici svoje fiksne točke p-krat zvezno odvedljiva in $\lvert g'(\alpha) \rvert \leq 1, g^{(k)}(\alpha) = 0\text{ za }k = 1,\ldots,p-1 \text{ in }
    g^{p}(\alpha) \neq 0.$ Potem ima iterativna metoda lokalno red konvergence p.
\end{izrek}
\begin{proof}
    Razvijmo $g$ v Taylorjevo vrsto okrog $\alpha$
    \[
        x_{r+1} = g(x_{r}) = \alpha + \frac{1}{p!}(x_{r} - \alpha)^{p}g^{(p)}(\xi),
    \]
    kjer $\xi$ leži med $x_{r}$ in $\alpha.$ Ocenimo odvod navzgor in navzdol ter s tem dobimo konstanti $C_{1},C_{2}$ iz \ref{red_konv}.
\end{proof}

Pa poskusimo sedaj določiti red konvergence Newtonove metode. Označimo
\[
    g(x) = x - \frac{f(x)}{f'(x)}.
\]
Odvajamo in dobimo
\[
    g'(x) = \frac{f(x)f''(x)}{f'^{2}(x)}.
\]
Vidimo, da je potrebno ločiti dva primera:
\begin{itemize}
    \item Če je $g'(\alpha) = 0,$ torej je $\alpha$ enostavna ničla je konvergenca vsaj kvadratična. Z nadaljnjim računom dobimo
    \[
        g''(\alpha) = \frac{f''(\alpha)}{f'(\alpha)},
    \]
    in vidimo, da je pri $f''(\alpha) \neq 0$ konvergenca kvadratična. Sicer postopek nadaljujemo, dokler ne najdemo prvega odvoda z vrednostjo v $\alpha$ 0.
    \item Če je $\alpha$ m-kratna ničla pa se da pokazati
    \[
        \lim_{x\to\alpha} g'(x) = 1 - \frac{1}{m},
    \]
    od koder sledi linearna konvergenca.
\end{itemize}
\subsubsection{Newton--Raphsonova metoda v višjih dimenzijah}
Newtonovo metodo se da enostavno posplošiti za iskanje ničel vektorskih funkcij. Recimo, da imamo funkcijo $F:\mathbb{R^n} \rightarrow \mathbb{R^n}$ in iščemo tak
vektor $x^{*} \in \mathbb{R^n},$ za katerega bo $F(x^{*}) = (f_{1}(x^{*}_{1}),\ldots,f_{n}(x^{*}_{n})) = (0,\ldots,0).$

Podobno kot zgoraj tvorimo zaporedje
\[
    x^{(r+1)} = x^{(r)} - JF(x^{(r)})^{-1}F(x^{(r)}),~r=0,1,\ldots
\]
kjer smo z \textit{JF} označili Jacobijevo matriko. V praksi njenega odvoda ne računamo, temveč uvedemo premike in rešujemo sistem enačb
\begin{align*}
    JF(x^{(r)}) h &= -F(x^{(r)}),\\
    x^{(r+1)} &= x^{(r)} + h,~r=0,1,\ldots
\end{align*}
od koder izračunamo vektor premikov $h \in \mathbb{R^n}$ in nato posodobimo prejšnji približek.

Izpeljava je podobna tisti v eni dimenziji, preko razvoja v Taylorjevo vrsto. Recimo, da so vse komponente funkcije $F$ dvakrat zvezno odvedljive v okolici rešitve. Potem razvijemo
\[
    f_{i}(x+h) = f_{i}(x) + \sum_{k=1}^{n} \frac{\partial f_{i}(x)}{\partial x_{k}}*h_{k} + \ldots,~i=1,\ldots,n
\]
Želimo, da bo $f_{i}(x+h) = 0$ za vsak $i$, zanemarimo člene od kvadratnega dalje in rešujemo sistem za premike
\[
    \begin{bmatrix}
        \frac{\partial f_{1}(x)}{\partial x_{1}} & \cdots & \frac{\partial f_{1}(x)}{\partial x_{n}} \\
        \vdots & & \vdots \\
        \frac{\partial f_{n}(x)}{\partial x_{1}} & \cdots & \frac{\partial f_{n}(x)}{\partial x_{n}}
    \end{bmatrix}
    \begin{bmatrix}
        h_{1} \\
        \vdots\\
        h_{n}
    \end{bmatrix} 
    = -
    \begin{bmatrix}
       f_{1}(x) \\
       \vdots \\
       f_{n}(x)
    \end{bmatrix}.
\]
V okolici rešitve enačbe ima metoda kvadratično konvergenco, težava pa je v iskanju začetnega približka.
\subsection{Fisher's scoring}
Fisher's scoring algoritem je variacija v prejšnjem razdelku opisanega Newton -- Raphsonovega algoritma, ki se v statistiki uporablja za numerično reševanje enačb največjega 
verjetja. Poimenovana je po Ronaldu Fisherju, enem najpomembenjših angleških statistikov dvajsetega stoletja.

Ponovimo najprej nekaj terminologije. Funkcija zbira je gradient logaritemske funkcije verjetja po ocenjevanem parametru.
Informacijska (oziroma Fisherjeva Informacijska) matrika (angl. \textit{(Fisher) information matrix}) je definirana kot 
\[
    \mathrm{FI}(\theta) = \mathbb{E}\left(\left(\frac{\partial}{\partial\theta} L(\theta)\right) \left(\frac{\partial}{\partial\theta} L(\theta)\right)^\top \right) %= \mathrm{E}[\dot{\ell}(\theta) \dot{\ell}(\theta)^\top].
\]
Fisher scoring algoritem je po zgornjih oznakah 
\begin{align}
    \theta{n + 1} = \theta_{n} - FI(\theta)^{-1}\nabla L(\theta)
\end{align}
%\subsubsection{Fisherjeva informacijska matrika in informacijska enakost}
%V sledečem razdelku bomo utemeljili uporabo Fisher scoring algoritma in prikazali njegovo glavno prednost pred običajno Newtonovo iteracijo, opisano v poglavju \ref{nr}.
%
%Ob določenih predpostavkah o gladkosti logaritemske funkcije verjetja (ki za eksponentno družino vedno držijo), bomo sedaj pokazali, da je pričakovana vrednost
%zbirne funkcije enaka nič. Funkcijo verjetja tu pišemo kot logaritem gostote slučajnega vektorja.
%\begin{align} \label{Ezbirne}
%    \mathrm{E}[\nabla L (\theta)] &= \int f_{\theta}(x) \nabla_{\theta}\log(f_{\theta}(x)) \,dx = \int f_{\theta}(x) \frac{\nabla f_{\theta}(x)}{f_{\theta}(x)} \,dx \nonumber \\
%        &= \int \nabla f_{\theta}(x) \,dx \overset{*}{=} \nabla \int f_{\theta}(x) \,dx = \nabla 1 = 0, 
%\end{align}
%kjer smo v enakosti $(*)$ zamenjali integracijo in odvajanje, kar po teoriji mere smemo - gostota porazdelitev eksponentne družine je zvezno odvedljiva. 
%Velja tudi
%\[
%    \nabla^{2} \log{f_{\theta}(x)} = \frac{\nabla^2 f_{\theta}(x)}{f_{\theta}(x)} - \frac{\nabla f_{\theta}(x) \nabla f_{\theta}(x)^\top }{f_{\theta}(x)^{2}} = 
%    \frac{\nabla^2 f_{\theta}(x)}{f_{\theta}(x)} - \dot{\ell}(\theta)\dot{\ell}(\theta)^\top,
%\]
%kjer smo v zadnji enakosti upoštevali
%\[
%    \nabla L (\theta) = \nabla \log(f_{\theta}(x)) = \frac{\nabla f_{\theta}(x)}{f_{\theta}(x)} 
%\]
%Povzemimo vse, kar smo dokazali v prejšnjih enačbah in imamo
%\begin{align}\label{infoeq}
%    I(\theta) &= \mathrm{E}[\nabla L(\theta)(\nabla L(\theta))^\top] =  -\int f_{\theta}(x) \nabla^2 \log f_{\theta}(x) \,dx+ \int \nabla^2 f_{\theta}(x) \,dx \nonumber \\
%    &= -\mathrm{E}[\nabla^2 \log f_{\theta}(x)] + \nabla^2\int \log f_{\theta}(x) \,dx = -\mathrm{E}[\nabla^2 \log f_{\theta}(x)]
%\end{align}
%Enačbi \eqref{infoeq} rečemo tudi \textit{informacijska enakost}. To nam bo koristilo pri dokazovanju enakosti med Newton -- Raphson algoritmom in Fisher scoringom za logistični model.
%Kot smo videli v prejšnjih poglavjih, želimo imeti pozitivno semi-definitno matriko in s tem monoton algoritem. Izkaže se, da je informacijska matrika ravno variančno-kovariančna matrika
%zbirne funkcije. Po enačbi \eqref{Ezbirne} se spomnimo, da je pričakovana vrednost zbirne funckije enaka 0. Potem takoj sledi
%\begin{align}
%    I(\theta) &= \mathrm{E}[\nabla L(\theta)\nabla L(\theta)^\top] \nonumber\\
%    &= \mathrm{E}[\left(\nabla L(\theta) - \mathrm{E}[\nabla L(\theta)]\right)\left(\nabla L(\theta) - \mathrm{E}[\nabla L(\theta)]\right)^\top] \nonumber \\
%    &= \mathrm{Var}[\nabla L(\theta)].
%\end{align}
%Variančno-kovariančne matrike pa so pozitivno semi-definitne.

\subsubsection{Ujemanje Newton-Raphson in Fisher's scoring za kanonične povezovalne funkcije}
Spomnimo se zaključkov poglavja \ref{kan}. Tam smo dokazali, da za modele s kanonično povezovalno funkcijo velja $\mathbb{E}(\frac{\partial^2}{\partial\theta^2}L) = \frac{\partial^2}{\partial\theta^2},$
po informacijski enakosti pa velja 
\begin{equation*}
    \mathrm{FI}(\theta) = \mathbb{E}\left((\nabla L)(\nabla L)^\top\right) = \mathbb{E}(\frac{\partial^2}{\partial\theta^2}L) = \frac{\partial^2}{\partial\theta^2}L \\
    \rightarrow \mathrm{FI}(\theta) = \frac{\partial^2}{\partial\theta^2}L,
\end{equation*}
torej Fisherjeva informacija je za kanonične modele enaka Hessejevi matriki logaritemske funkcije verjetja! Poleg tega pa velja še
\begin{align}
    \mathrm{FI}(\theta) &= \mathrm{E}[(\nabla L(\theta))(\nabla L(\theta))^\top] \nonumber\\
    &= \mathrm{E}[\left(\nabla L(\theta) - \mathrm{E}[\nabla L(\theta)]\right)\left(\nabla L(\theta) - \mathrm{E}[\nabla L(\theta)]\right)^\top] \nonumber \\
    &= \mathrm{Var}[\nabla L(\theta)],
\end{align}
variančno kovariančne matrike pa so pozitivno semidefinitne, kar pomeni da imamo konstanten algoritem.


Povzemimo; za kanonične povezovalne funkcije smo dokazali enakost med Fisherjevo informacijo in Hessejevo matriko. S tem se v enem koraku izognemo računanju matrike drugih 
odvodov in pridobimo pozitivno semidefinitno matriko v imenovalcu. Koristi uporabe kanoničnih povezovalnih funkcij so toraj očitne.

\subsubsection{Fisher's scoring v logističnem modelu}
Poglejmo za trenutek nazaj v poglavje \ref{ocenpar}, natančneje k enačbam \eqref{ell},\eqref{prvi} in \eqref{drugi}. Iz prejšnjega razdelka vemo tudi, da velja
\[
    \mathrm{FI}(\theta) = \mathbb{E}[-\nabla ^2 L(\theta)] \overset{\eqref{drugi}}{=} X^\top v(\theta)X = -\nabla ^2 L(\theta),
\]
kjer smo seveda uporabili tudi prej dokazano informacijsko enakost. Tako vidimo, da Fisher's scoring in Newton--Raphsonova metoda v primeru logistične regresije
res sovpadata, saj je matrika drugih odvodov ravno enaka informacijski matriki. Če zapišemo sedaj vse skupaj
\begin{align}
    \hat{\theta}_{i+1} &= \hat{\theta}_{i} - (\nabla^{2} L(\hat{\theta}_{i}))^{-1} \nabla L(\hat{\theta}_{i}) \nonumber \\
    &= \hat{\theta}_{i} + (\mathbf{X}^\top v(\hat{\theta}_{i}) \mathbf{X})^{-1}\mathbf{X}^\top(y - \mu(\hat{\theta}_{i})) 
\end{align}

\section{Primeri}
\subsection{Ocenjevanje parametrov v logističnem modelu}

V praktično usmerjenem delu naloge smo v Pythonu implementirali zgoraj opisani postopek Fisher scoring algoritma za binomsko porazdeljene slučajne
spremenljivke. Za delo v Pythonu smo uporabili več knjižnic\; \texttt{NumPy} za računanje z matrikami in vektorji, reševanje sistemov enačb ter invertiranje,
knjižnico \texttt{pandas} za uvoz podatkov in njihovo začetno urejanje. Na koncu smo si s paketom \texttt{Pyplot} iz knjižnice \texttt{Matplotlib} rezultate izrisali.
V implementaciji smo popolnoma sledili zgoraj izpeljanim enačbam, zato jih tu nebomo ponovno navajali.

\subsubsection{Algoritem za logistični model in rezultati}
Povežimo vso izpeljano teorijo v algoritem za ocenjevanje parametrov modela oblike
\[
    \mathrm{logit}(p_{i}) = X\beta,
\]
torej kanoničnega logističnega modela.
\begin{algorithm}[H]
    \caption{\textbf{function} LogitModel(iteracije, X, Y, $\beta_{zacetni}$, $\epsilon$)}
\begin{algorithmic}
    
    %\COMMENT 
    \STATE $p = \frac{\exp{(X^\top \beta_{zacetni})}}{1 + \exp{(X^\top \beta_{zacetni})}}$
    \STATE $V = p(1 - p)$ 
    \STATE $Score = X^\top (Y - p)$ 
    \STATE $Info = X^\top V X$ 
    
    \STATE \text{\textit{Reši sistem na h:}} $Info * h = Score$
    \STATE $\beta_{star} = \beta_{zacetni}$
    \STATE $\beta_{nov} = \beta_{star} + h$
    \WHILE{$i \leq \text{iteracije}$}
        
        \IF{$\beta_{nov} - \beta_{star} \geq \epsilon$}
            \STATE $p = \frac{\exp{(X^\top \beta_{nov})}}{1 + \exp{(X^\top \beta_{nov})}}$ 
            \STATE $V = p(1 - p)$ 
            \STATE $Score = X^\top (Y - p)$ 
            \STATE $Info = X^\top V X$ 
            \STATE \text{\textit{Razreši na h:}}
            \STATE $Info * h = Score$ 
            \STATE $\beta_{star} = \beta_{nov}$ 
            \STATE $\beta_{nov} = \beta_{star} + h$ 
        \ELSE
        \STATE \text{\textit{Dosegli smo želeno natančnost v zadostnem številu korakov}}
        \RETURN $\beta_{nov}$
        \ENDIF
        
    \ENDWHILE
\end{algorithmic}
\end{algorithm}

Algoritma pa ne bomo samo navedli, preizkusili ga bomo na konkretnih podatkih. Zanimalo nas bo, kako sta temperatura in pritisk vplivala na odpoved
tesnil na vesoljskih misijah preden je v veljavo stopil \textit{Challenger}. Najprej bomo pogledali le enodimenzionalne ocene, potem pa vse skupaj združili.

Spodaj je tabela; tej podatki so shranjeni v matriki $X,$ le da je tam dodan prvi stolpec enic - za izračun $\beta_{0},$ ki nastopa brez pojasnjevalne spremenljivke.
\textit{POLET} označuje zaporedno številko poleta, \textit{TEMPERATURA} in \textit{PRITISK} sta temperatura in pritisk v okolici, \textit{TESNILO} pa je binarna
spremenljivka - 1 označuje, da je tesnilo popustilo, 0 pa da je pogoje vzdržalo.
\begin{center}
\begin{table}[H]
    \begin{tabular}{| c | c | c | c |}
        \hline
        POLET & TEMPERATURA & PRITISK & TESNILO \\
        \hline
        1&66&50&0\\
        2&70&50&1\\
        3&69&50&0\\
        4&68&50&0\\
        5&67&50&0\\
        6&72&50&0\\
        7&73&100&0\\
        8&70&100&0\\
        9&57&200&1\\
        10&63&200&1\\
        11&70&200&1\\
        12&78&200&0\\
        13&67&200&0\\
        14&53&200&1\\
        15&67&200&0\\
        16&75&200&0\\
        17&70&200&0\\
        18&81&200&0\\
        19&76&200&0\\
        20&79&200&0\\
        21&75&200&1\\
        22&76&200&0\\
        23&58&200&1\\
        \hline
\end{tabular}
\caption{\label{podatki} Podatki uporabljeni v analizi}
\end{table}
\end{center}

Poženimo algoritem najprej le na podatkih o temperaturi. Za začetno vrednost približka $\beta$ vzemimo 0, natančnost $\epsilon$ pa si predipšemo na 0,001.
Ker imamo le eno pojasnjevalno spremenljivko uporabljamo torej model oblike
\[
    \mathrm{logit}(p_{i}) = \beta_{0} + x_{i}\beta_{1}
\]
Za izračun približka znotraj 1 tisočinke, z začetno vrednostjo $\beta = (0,0),$ smo potrebovali le nekaj korakov. Vstavimo v invertirano logit transformacijo in dobimo
\[
    p = \frac{e^{15.04290 - 0.23216x}}{1+e^{15.04290 - 0.23216x}},
\]
torej $\beta_{0} = 15.04290$ in $\beta_{1} = -0.23216.$ Če graf narišemo, dobimo
\begin{center}
    \begin{figure}[H]
    \begin{tikzpicture}
        \begin{axis}[
            axis lines = center,
            %xtick = {-10,-9,-8,...,9,10}
            ytick = {0,0.1,0.2,...,1,1.1,1.2,1.3,1.4},
            %ylabel = verjetnost,
            %y label style = {at={(axis description cs:0.25,.5)},rotate=90}%,anchor=south}
        ]
        \addplot [
            domain=40:90, 
            samples=100, 
            color=black,
            ]
            {exp(15.04290 - (0.23216 * x))/(1 + exp(15.04290 - (0.23216 * x)))};
        %\addlegendentry{$x^2 + 2x + 1$}
        
        \end{axis}
    \end{tikzpicture}
    \caption{Izračunane verjetnosti z eno pojasnjevalno spremenljivko}
    \label{fig:logit1}
    \end{figure}
    \end{center}

Vidimo da je sigmoida v tem primeru obrnjena drugače kot na sliki \ref{fig:sigmoid} - tak rezultat nam da negativen predznak parametra $\beta_{1}.$
Sklepamo lahko torej da pri višjih temperaturah tesnila redkeje odpovejo. Za računanje smo si predpisali natančnost $\epsilon = 0,001,$ razlika med dvema zaporednima približkoma
ne sme presegati te vrednosti. Z uporabo našega algoritma (ter vgrajenih funkcij za invertiranje matrik in reševanje sistemov enačb) smo do rešitve prišli v vsega štirih iteracijah.

Enako rešimo še primer z dvema pojasnjevalnima spremenljivkama, pritisku in temperaturi. Rezultat je ploskev, saj nas zanima odpoved pri vsakem možnem
paru temperature in pritiska. Izračunani parametri znašajo $\beta_{0} = 13.29236,~\beta_{1} = -0.22867,~\beta_{2} = -0.01040.$ Prikazan je graf funkcije
\[
    \frac{e^{13.29236 -0.22867*x-0.01040*y}}{1+e^{(13.29236 -0.22867*x-0.01040*y)}}.
\]
\begin{center}
\begin{figure}[H]
\begin{tikzpicture}
    \begin{axis}
    \addplot3[
        surf,
        domain=30:100
    ]
    {exp(13.29236 -0.22867*x-0.01040*y)/(1+exp(13.29236 -0.22867*x-0.01040*y))};
    \end{axis}
\end{tikzpicture}
\caption{Izračunane verjetnosti z dvema pojasnjevalnima spremenljivkama}
\end{figure}
\end{center}
Zopet smo algoritem pognali na enakih začetnih vrednostih približka in zahtevane natančnosti, rešitev pa dobili po štirih iteracijah.

\subsection{Ocenjevanje parametrov v probit modelu}
Sedaj ponovimo postopek še s probit modelom. Zaradi neoptimizirane numerične metode pričakujemo večje število iteracij za doseženo želeno natančnost.

\subsubsection{Algoritem za probit model}
Najprej postavimo algoritem. Ta se idejno sicer ne bo bistveno razlikoval od tistega uporabljenega v prejšnjem odseku, vendar pa je očitno precej zapletenejši za računanje.
Posebej smo označili \textit{koeficiente} pri Hessejevi matriki in funkciji zbira - to so le deli dejanske formule, izračunani posebej za večjo preglednost.

\begin{algorithm}[H]
    \caption{\textbf{function} ProbitModel(iteracije, X, Y, $\beta_{zacetni}$, $\epsilon$)}
\begin{algorithmic}

    \STATE $p = \Phi(X^\top \beta_{zacetni})$
    \STATE $Score_{koef} = \frac{Y-p}{p*(1-p)} * \phi(X^\top \beta_{zacetni})$
    \STATE $Score = X^\top Score_{koef}$

    \STATE $Hess_{koef} = \frac{\phi(X^\top \beta_{zacetni})}{p*(1-p)} * \left(\phi(X^\top \beta_{zacetni})\frac{2*p*Y - p^2-Y}{p*(1-p)} - X(Y - p)\right)$
    \STATE $H = X^\top Hess_{koef} X$
        
    \STATE \text{\textit{Reši sistem na h:}} $H * h = Score$
    \STATE $\beta_{star} = \beta_{zacetni}$
    \STATE $\beta_{nov} = \beta_{star} - h$
    \WHILE{$i \leq \text{iteracije}$}
        
        \IF{$\beta_{nov} - \beta_{star} \geq \epsilon$}
            \STATE $p = \Phi(X^\top \beta_{nov})$
            \STATE $Score_{koef} = \frac{Y-p}{p*(1-p)} * \phi(X^\top \beta_{nov})$
            \STATE $Score = X^\top Score_{koef}$
            
            \STATE $Hess_{koef} = \frac{\phi(X^\top \beta_{nov})}{p*(1-p)} * \left(\phi(X^\top \beta_{nov})\frac{2*p*Y - p^2-Y}{p*(1-p)} - X(Y - p)\right)$
            \STATE $H = X^\top Hess_{koef} X$
            \STATE \text{\textit{Razreši na h:}}
            \STATE $H * h = Score$ 
            \STATE $\beta_{star} = \beta_{nov}$ 
            \STATE $\beta_{nov} = \beta_{star} - h$ 
        \ELSE
        \STATE \text{\textit{Dosegli smo želeno natančnost v zadostnem številu korakov}}
        \RETURN $\beta_{nov}$
        \ENDIF
        
    \ENDWHILE
\end{algorithmic}
\end{algorithm}
Ponovimo postopek in za oceno le z eno pojasnjevalno spremenljivko, po vsega petih iteracijah dobimo $\beta_{0} = 8.77495,~\beta_{1} = -0.13510$ in funkcijo
\[
    \frac{e^{8.77495--0.13510x}}{1+e^{8.77495-0.13510x}}.
\]
Za vključeni obe spremenljivki pa $\beta_{0} = 8.08004,~\beta_{1} = -0.13774,~\beta_{2} = -0.006014$

Torej je predvidevanje, da bomo potrebovali bistveno več iteracij napačna, saj je bila potrebna le ena več kot pri logistični regresiji. Ugibamo, da do tega pride zaradi
majhne količine testnih podatkov in bi se pri večjih podatkovnih bazah razlike primerno povečale.

\subsection{Primerjava logit in probit modela}
Zanimivejše kot le naštevanje parametrov, pa je primerjanje dobljenih rezultatov.
\begin{center}
    \begin{figure}[H]
    \begin{tikzpicture}
        \begin{axis}[
            axis lines = center,
            %xtick = {-10,-9,-8,...,9,10}
            ytick = {0,0.1,0.2,...,1,1.1,1.2,1.3,1.4},
            %ylabel = verjetnost,
            %y label style = {at={(axis description cs:0.25,.5)},rotate=90}%,anchor=south}
        ]
        \addplot [
            domain=25:110, 
            samples=100, 
            color=black,
            ]
            {exp(15.04290 - (0.23216 * x))/(1 + exp(15.04290 - (0.23216 * x)))};
        \addplot[
            domain=25:110, 
            samples=100,
            color = red,
            ]
            {exp(8.77495 - (0.13510 * x))/(1 + exp(8.77495 - (0.13510 * x)))};
        %\addlegendentry{$x^2 + 2x + 1$}
        \addlegendentry{logit}
        \addlegendentry{probit}
        \end{axis}
    \end{tikzpicture}
    \caption{Izračunane verjetnosti z eno pojasnjevalno spremenljivko}
    \label{fig:logit1}
    \end{figure}
    \end{center}

Z rdečo je na zgornjem grafu narisana sigmoida, dobljena iz probit modela, s črno pa iz logističnega. Kljub temu, da se število iteracij za njun izračun ne razlikuje
bistvemo, bi v tem primeru raje izbrali logistični model. Razlog tiči v hitrosti padanja, ki je višja pri slednjem, kar nam koristi pri nadaljnji analizi.

% slovar
\section*{Slovar strokovnih izrazov}

\geslo{generalized linear model}{posplošeni linearni model}\\
\geslo{score function}{funkcija zbira}\\
\geslo{Fisher information matrix}{Fisherjeva informacijska matrika}\\
\geslo{link function}{povezovalna funkcija}\\
\geslo{maximum likelihood estimator (MLE)}{cenilka največjega verjetja}\\
\geslo{exponential family}{eksponentna družina}
%
%
% seznam uporabljene literature
%\begin{thebibliography}{99}
%\bibitem{}
%end{thebibliography}
\nocite{*}
\bibliographystyle{plain}
\bibliography{literatura}



\end{document}
